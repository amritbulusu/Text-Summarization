{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fndata = \"C:/Users/amrit/OneDrive/Desktop/Thesis/signalmedia/sample-1M.jsonl\"\n",
    "\n",
    "heads = []\n",
    "desc = []\n",
    "keywords = []\n",
    "counter = 0\n",
    "with open(fndata) as f:\n",
    "        for line in f:\n",
    "            if counter < 50000:\n",
    "                jdata = json.loads(line)\n",
    "                heads.append(jdata[\"title\"].lower())\n",
    "                desc.append(jdata[\"content\"].lower())\n",
    "                counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__pickleStuff(\"desc.p\",desc)\n",
    "__pickleStuff(\"heads.p\", heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'game of thrones' creators 'never set out to offend anyone'\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fifth season of hbo\\'s \"game of thrones\" made history at the emmy awards \\xa0on sunday, when it became the first sci-fi or fantasy show ever to win best drama and its 12 total awards broke the record for the most won by any show in a single year.\\xa0 \\n \\nyet some have griped \\xa0that, even if \"game of thrones\" is a great show, this wasn\\'t its best season . indeed, that this may even have been its worst season . season 5 was almost certainly its most controversial season, especially because of the (spoiler alert) the marital rape \\xa0of sansa stark in episode 6 and the sacrificial burning of shireen baratheon \\xa0in episode 9, neither of which has yet happened in george r.r. martin\\'s novels. \\n \\nsome of the show\\'s creatives, including the director of episode 6 , have addressed\\xa0these controversies in the past. but evidently, \"game of thrones\" creators dan benioff and d.b. weiss still feel the issue is unresolved. in a post-emmy interview with entertainment weekly , benioff said that they \"never set out to offend anyone\" by staging those brutal moments. \\n \\n\"to try to offend anyone would be juvenile, but to be afraid to offend people would be cowardly,\" benioff explained. \"there were people saying they were never going to watch again. we’re just trying to tell the story the best we can.\" \\n \\nit was certainly good enough for emmy voters! and for quite a few viewers at home \\xa0as well.\\xa0 \\n \\nalso on huffpost'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(desc)):\n",
    "    documents.append(desc[i].split(\"\\n \\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from stemming.porter2 import stem\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text, stopwords_removal=False, stemming=False):\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    str1 = re.findall(\"[@a-zA-Z0-9]* '[a-zA-Z]\", text)\n",
    "    for l in str1:\n",
    "        text= re.sub(l,l.replace(\" \",\"\"),text)\n",
    "    text = re.findall(r\"[\\w']+\", text)\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in contractions:\n",
    "            new_text.append(contractions[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    text = \" \".join(new_text)\n",
    "    tokens = text.split()\n",
    "    words = []\n",
    "    if stopwords_removal:\n",
    "        for token in tokens:\n",
    "            if token not in cachedStopWords:\n",
    "                words.append(token)\n",
    "            text = \" \".join(words)        \n",
    "    if stemming:\n",
    "        text = [\" \".join([stem(word) for word in sentence.split(\" \")]) for sentence in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    for j in range(len(documents[i])):\n",
    "        documents[i][j] = preprocess(documents[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fifth season of hbo\\'s \"game of thrones\" made history at the emmy awards \\xa0on sunday, when it became the first sci-fi or fantasy show ever to win best drama and its 12 total awards broke the record for the most won by any show in a single year.\\xa0 \\n \\nyet some have griped \\xa0that, even if \"game of thrones\" is a great show, this wasn\\'t its best season . indeed, that this may even have been its worst season . season 5 was almost certainly its most controversial season, especially because of the (spoiler alert) the marital rape \\xa0of sansa stark in episode 6 and the sacrificial burning of shireen baratheon \\xa0in episode 9, neither of which has yet happened in george r.r. martin\\'s novels. \\n \\nsome of the show\\'s creatives, including the director of episode 6 , have addressed\\xa0these controversies in the past. but evidently, \"game of thrones\" creators dan benioff and d.b. weiss still feel the issue is unresolved. in a post-emmy interview with entertainment weekly , benioff said that they \"never set out to offend anyone\" by staging those brutal moments. \\n \\n\"to try to offend anyone would be juvenile, but to be afraid to offend people would be cowardly,\" benioff explained. \"there were people saying they were never going to watch again. we’re just trying to tell the story the best we can.\" \\n \\nit was certainly good enough for emmy voters! and for quite a few viewers at home \\xa0as well.\\xa0 \\n \\nalso on huffpost'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the fifth season of hbo s game of thrones made history at the emmy awards on sunday when it became the first sci fi or fantasy show ever to win best drama and its 12 total awards broke the record for the most won by any show in a single year',\n",
       " 'yet some have griped that even if game of thrones is a great show this wasn t its best season indeed that this may even have been its worst season season 5 was almost certainly its most controversial season especially because of the spoiler alert the marital rape of sansa stark in episode 6 and the sacrificial burning of shireen baratheon in episode 9 neither of which has yet happened in george r r martin s novels',\n",
       " 'some of the show s creatives including the director of episode 6 have addressed these controversies in the past but evidently game of thrones creators dan benioff and d b weiss still feel the issue is unresolved in a post emmy interview with entertainment weekly benioff said that they never set out to offend anyone by staging those brutal moments',\n",
       " 'to try to offend anyone would be juvenile but to be afraid to offend people would be cowardly benioff explained there were people saying they were never going to watch again we re just trying to tell the story the best we can',\n",
       " 'it was certainly good enough for emmy voters and for quite a few viewers at home as well',\n",
       " 'also on huffpost']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__pickleStuff(\"docs1.p\",docs[:250000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__pickleStuff(\"docs2.p\",docs[250000:500000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__pickleStuff(\"docs3.p\",docs[500000:750000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__pickleStuff(\"docs4.p\",docs[750000:1000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for i in range(len(heads)):\n",
    "    summaries.append(preprocess(heads[i]))\n",
    "__pickleStuff(\"summaries.p\",summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff, protocol=4)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425495\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for document in documents:\n",
    "    for sentence in document:\n",
    "        c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs1 = __loadStuff(\"docs1.p\")\n",
    "# docs2 = __loadStuff(\"/signalmedia/docs2.p\")\n",
    "# docs3 = __loadStuff(\"/signalmedia/docs3.p\")\n",
    "# docs4 = __loadStuff(\"/signalmedia/docs4.p\")\n",
    "headlines = __loadStuff(\"summaries.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = docs1+docs2+docs3+docs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del docs1, docs2, docs3, docs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, documents):\n",
    "    for sentence in documents:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary : 1513663\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, documents)\n",
    "count_words(word_counts, headlines)\n",
    "\n",
    "print(\"Size of vocabulary :\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 1917248\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('/numberbatch/numberbatch.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 168034\n",
      "Percent of words that are missing from vocabulary: 11.1%\n"
     ]
    }
   ],
   "source": [
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count>threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "missing_ratio = round(missing_words/len(word_counts), 4)*100\n",
    "\n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inhale', 345),\n",
       " ('methamphetamine', 1130),\n",
       " ('3317', 33),\n",
       " ('nismo', 139),\n",
       " ('critically', 3906),\n",
       " ('rabin', 301),\n",
       " ('promise', 14661),\n",
       " ('enghouse', 132),\n",
       " ('blocks', 10911),\n",
       " ('663', 558),\n",
       " ('foldscope', 80),\n",
       " ('relion', 24),\n",
       " ('relaxants', 36),\n",
       " ('rakity', 25),\n",
       " ('edenderry', 35),\n",
       " ('kawashima', 28),\n",
       " ('infento', 67),\n",
       " ('caserta', 66),\n",
       " ('forgivable', 65),\n",
       " ('tgyg', 28),\n",
       " ('stormtracker', 88),\n",
       " ('macklin', 140),\n",
       " ('lumithera', 31),\n",
       " ('repeating', 2028),\n",
       " ('6940', 24),\n",
       " ('versartis', 177),\n",
       " ('6218', 23),\n",
       " ('purifier', 277),\n",
       " ('dolci', 36),\n",
       " ('dona', 103)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_words = []\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold and word not in embeddings_index:\n",
    "        missing_words.append((word,count))\n",
    "missing_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 1513663\n",
      "Number of words we will use: 173024\n",
      "Percent of words we will use: 11.43%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {}\n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count>= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173024\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return(ints, word_count, unk_count)# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in documents: 255656477\n",
      "Total number of UNKs in documents: 3907520\n",
      "Percent of words that are UNK: 1.53%\n"
     ]
    }
   ],
   "source": [
    "int_summaries, word_count, unk_count = convert_to_ints(headlines, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(documents, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in documents:\", word_count)\n",
    "print(\"Total number of UNKs in documents:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counts\n",
       "0      10\n",
       "1      15\n",
       "2      13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lengths(int_summaries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "               counts\n",
      "count  1000000.000000\n",
      "mean         9.667398\n",
      "std          4.421006\n",
      "min          0.000000\n",
      "25%          7.000000\n",
      "50%          9.000000\n",
      "75%         12.000000\n",
      "max         83.000000\n",
      "\n",
      "Texts:\n",
      "               counts\n",
      "count  1000000.000000\n",
      "mean       246.989079\n",
      "std        288.868089\n",
      "min          1.000000\n",
      "25%         80.000000\n",
      "50%        183.000000\n",
      "75%        327.000000\n",
      "max      17266.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502.0\n",
      "653.0\n",
      "1179.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "17.0\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min(lengths_texts.counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614945\n",
      "614945\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 1179\n",
    "max_summary_length = 24\n",
    "min_length = 2\n",
    "unk_text_limit = 2\n",
    "unk_summary_limit = 1\n",
    "\n",
    "def filter_condition(item):\n",
    "    int_summary = item[0]\n",
    "    int_text = item[1]\n",
    "    if(len(int_summary) >= min_length and \n",
    "       len(int_summary) <= max_summary_length and \n",
    "       len(int_text) >= min_length and \n",
    "       len(int_text) <= max_text_length and\n",
    "       unk_counter(int_summary) <= unk_summary_limit and \n",
    "       unk_counter(int_text) <= unk_text_limit):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "int_text_summaries = list(zip(int_summaries , int_texts))\n",
    "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
    "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\n",
    "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
    "sorted_summaries = list(sorted_int_text_summaries[0])\n",
    "sorted_texts = list(sorted_int_text_summaries[1])\n",
    "# Delete those temporary varaibles\n",
    "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_texts = [len(text) for text in sorted_texts]\n",
    "lengths_texts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# int_to_vocab = __loadStuff(\"int_to_vocab.p\")\n",
    "# sorted_summaries = __loadStuff(\"sorted_summaries.p\")\n",
    "sorted_texts = __loadStuff(\"sorted_texts.p\")\n",
    "# vocab_to_int = __loadStuff(\"vocab_to_int.p\")\n",
    "# word_embedding_matrix = __loadStuff(\"word_embedding_matrix.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # original code is missing this line below, that is how we connect layers \n",
    "            # by feeding the current layer's output to next layer's input\n",
    "            rnn_inputs = enc_output\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
    "                            vocab_size, max_summary_length,batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<GO>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PAD>' has id: 173021\n",
      "pad summaries batch samples:\n",
      "\r",
      " [[172893 172872    200 172882   2031 172880 172879    772]\n",
      " [   421 156966 107178 173021 173021 173021 173021 173021]\n",
      " [   421 156966 107952 173021 173021 173021 173021 173021]\n",
      " [   421 156966  84561 173021 173021 173021 173021 173021]\n",
      " [ 16491   2493 173020 172883  69582 173021 173021 173021]]\n",
      "it\n",
      "s\n",
      "time\n",
      "to\n",
      "act\n",
      "on\n",
      "our\n",
      "watch\n",
      "daily\n",
      "picdump\n",
      "1450\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "daily\n",
      "picdump\n",
      "1443\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "daily\n",
      "picdump\n",
      "1448\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "prep\n",
      "volleyball\n",
      "<UNK>\n",
      "at\n",
      "durand\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n"
     ]
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
    "sorted_summaries_samples = sorted_summaries[7:50]\n",
    "sorted_texts_samples = sorted_texts[7:50]\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
    "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))\n",
    "for i in range(len(pad_summaries_batch_samples)):\n",
    "    for j in range(len(pad_summaries_batch_samples[i])):\n",
    "        print(int_to_vocab[pad_summaries_batch_samples[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "# reset_graph()\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 26\n",
      "The longest text length: 39\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 50000\n",
    "end = start + 50000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,173025] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like/Enter, ^optimization/gradients/Sub)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: optimization/gradients/b_count_2/_289 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5008_optimization/gradients/b_count_2\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopoptimization/gradients/decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/Softmax_grad/mul/StackPopV2/_11)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like', defined at:\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-267d0a88e281>\", line 41, in <module>\n    gradients = optimizer.compute_gradients(cost)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 456, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 948, in _SelectGrad\n    zeros = array_ops.zeros_like(x)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1564, in zeros_like\n    return gen_array_ops._zeros_like(tensor, name=name)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 8157, in _zeros_like\n    \"ZerosLike\", x=x, name=name)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'decode/decoder/while/Select_1', defined at:\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-267d0a88e281>\", line 21, in <module>\n    batch_size)\n  File \"<ipython-input-13-4248df55db9b>\", line 23, in seq2seq_model\n    num_layers)\n  File \"<ipython-input-20-ebe80d61463e>\", line 21, in decoding_layer\n    batch_size)\n  File \"<ipython-input-10-1ab0b7fdf441>\", line 15, in training_decoding_layer\n    maximum_iterations=max_summary_length)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 309, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 276, in body\n    zero_outputs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 387, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 387, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,173025] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like/Enter, ^optimization/gradients/Sub)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: optimization/gradients/b_count_2/_289 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5008_optimization/gradients/b_count_2\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopoptimization/gradients/decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/Softmax_grad/mul/StackPopV2/_11)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,173025] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like/Enter, ^optimization/gradients/Sub)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: optimization/gradients/b_count_2/_289 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5008_optimization/gradients/b_count_2\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopoptimization/gradients/decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/Softmax_grad/mul/StackPopV2/_11)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ee460aa25f1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                  \u001b[0msummary_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msummaries_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                  \u001b[0mtext_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtexts_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                  keep_prob: keep_probability})\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,173025] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like/Enter, ^optimization/gradients/Sub)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: optimization/gradients/b_count_2/_289 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5008_optimization/gradients/b_count_2\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopoptimization/gradients/decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/Softmax_grad/mul/StackPopV2/_11)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like', defined at:\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-267d0a88e281>\", line 41, in <module>\n    gradients = optimizer.compute_gradients(cost)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 456, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 948, in _SelectGrad\n    zeros = array_ops.zeros_like(x)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1564, in zeros_like\n    return gen_array_ops._zeros_like(tensor, name=name)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 8157, in _zeros_like\n    \"ZerosLike\", x=x, name=name)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'decode/decoder/while/Select_1', defined at:\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-267d0a88e281>\", line 21, in <module>\n    batch_size)\n  File \"<ipython-input-13-4248df55db9b>\", line 23, in seq2seq_model\n    num_layers)\n  File \"<ipython-input-20-ebe80d61463e>\", line 21, in decoding_layer\n    batch_size)\n  File \"<ipython-input-10-1ab0b7fdf441>\", line 15, in training_decoding_layer\n    maximum_iterations=max_summary_length)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 309, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 276, in body\n    zero_outputs)\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 387, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"C:\\Users\\amrit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 387, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,173025] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](optimization/gradients/decode/decoder/while/Select_1_grad/zeros_like/Enter, ^optimization/gradients/Sub)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: optimization/gradients/b_count_2/_289 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5008_optimization/gradients/b_count_2\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopoptimization/gradients/decode/decoder/while/BasicDecoderStep/decoder/attention_wrapper/Softmax_grad/mul/StackPopV2/_11)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = ' '.join(text)\n",
    "    tokens = text.split()\n",
    "    words = []\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    for token in tokens:\n",
    "        if token not in cachedStopWords:\n",
    "            words.append(token)\n",
    "    text = \" \".join(words)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \" \".join([int_to_vocab[i] for i in sorted_texts[500]])\n",
    "b = \" \".join([int_to_vocab[i] for i in sorted_texts[550]])\n",
    "\" \".join([int_to_vocab[i] for i in sorted_summaries[550]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentences = [a, b]\n",
    "generagte_summary_length =  [5,5]\n",
    "\n",
    "texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "if type(generagte_summary_length) is list:\n",
    "    if len(input_sentences)!=len(generagte_summary_length):\n",
    "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n",
    "    generagte_summary_length_list = generagte_summary_length\n",
    "else:\n",
    "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i, text in enumerate(texts):\n",
    "        generagte_summary_length = generagte_summary_length_list[i]\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "        # Remove the padding from the summaries\n",
    "        pad = vocab_to_int[\"<PAD>\"] \n",
    "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n",
    "        print(answer_logits)\n",
    "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [\" \".join([str(i) for i in sorted_texts[j]])for j in range(50000, 50500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['47499 2587 3104 43008 385 2133 634 385 693 43418 3648 5553 337 9116 8180 267 46396 21290 1305 22798 2133 634 501 488 52161 173022',\n",
       " '1485 1486 760 488 1328 3349 1488 347 351 348 760 7939 349 1328 3349 18 1325 916 2131 17311 7547 160 3918 173020 19281 173022',\n",
       " '4413 93 52220 5821 49 6766 8644 2427 1443 2170 753 505 304 489 1895 1895 536 5424 2037 753 753 304 489 1895 1895 173022',\n",
       " '7239 21585 94 301 3363 1340 301 773 1426 21585 89 773 1368 301 8839 1976 2024 1904 12092 1359 221 126 198 11380 4204 173022',\n",
       " '113 811 6404 3077 42959 15843 14377 3403 3077 42959 15843 14377 3403 42959 9395 14377 749 7868 3231 6654 814 3777 193 416 55 173022',\n",
       " '16472 2832 137877 3651 150079 10459 3638 184 6822 2025 17511 3784 274 9990 3829 536 413 8433 150079 5897 15172 12570 1072 10460 4599 173022',\n",
       " '173020 3141 11085 33582 1223 184 6481 30503 25417 40 1492 4295 8939 10394 931 173020 3141 683 488 266 6722 4993 8388 4705 2761 173022',\n",
       " '15765 194 193 5972 5975 12706 19051 17210 173020 17290 37754 45058 19051 11152 15043 10401 15775 7726 3591 16 47094 535 11856 3891 30979 173022',\n",
       " '2554 28 1207 749 22595 11384 2246 193 15206 25772 21213 610 20507 9452 4441 932 12893 25014 25436 1408 3700 1257 1664 612 488 173022',\n",
       " '1096 54 1436 8765 1791 19291 711 8409 677 5718 3014 144 3014 18 1076 7555 1436 20 5530 451 9067 8409 601 3579 11274 173022',\n",
       " '23829 753 170 304 52 180 501 2320 9553 618 14155 27332 10917 449 759 5622 5368 5222 4660 2195 2875 1110 3231 495 416 173022',\n",
       " '104 682 90 1169 319 218 11292 7 5523 1408 19918 682 429 444 1829 2521 105673 10953 1829 1131 1866 1107 1227 777 416 173022',\n",
       " '1692 1373 1072 2861 62512 12693 5251 14237 1584 2659 3185 517 12338 1333 1785 701 967 2909 12693 7318 14237 96 7332 3301 184 173022',\n",
       " '5524 4155 926 3493 9757 2417 6497 4688 7247 2109 1594 11730 400 601 10635 860 17210 48 397 86 316 2172 1769 1615 783 173022',\n",
       " '3132 3 591 23571 22077 0 835 6877 2640 31228 17054 10276 9229 3091 27558 3920 1997 2665 94 9481 7065 363 178 2333 683 173022',\n",
       " '23829 753 488 304 618 2185 1994 2320 9553 618 304 5878 316 5342 2907 10842 1729 173020 820 218 3835 1372 4801 5881 416 173022',\n",
       " '153 77614 11677 153 6327 4013 10829 2313 1328 1497 3638 4921 10733 144 618 66 173020 10829 2313 1328 1497 3638 4921 10733 144 173022',\n",
       " '2595 2169 6546 1291 10065 11213 3055 569 1697 451 4253 601 53940 1516 403 94 6844 44660 30756 1443 154806 22603 173020 26470 440 173022',\n",
       " '7316 1650 46 1614 3894 12266 61825 3583 22773 15944 2523 6358 4308 21499 789 2948 3426 851 937 21608 959 17882 2556 321 1615 173022',\n",
       " '111768 4207 1655 12603 3286 45149 22337 160 633 19368 3899 962 201 18395 45149 9190 418 469 9268 7467 3923 45149 15063 341 1154 173022',\n",
       " '7170 49928 374 8149 3163 2465 1485 495 7922 5652 5337 1485 3349 486 170 304 193 5535 2105 3364 5707 416 1874 123502 1443 173022',\n",
       " '1553 2016 9111 2139 35211 4197 118827 1270 1925 3963 153 10336 430 6225 6322 3 120 2037 3247 16592 508 23228 428 5020 1205 173022',\n",
       " '1095 61 4593 5027 6795 59390 410 23397 10242 173020 962 545 1355 520 539 833 4768 246 10368 4142 416 5707 416 1874 8087 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '1508 937 20084 2000 7 37965 20206 64007 866 2292 4436 1099 160 956 2827 64007 17125 1601 2937 93959 9120 363 3771 7122 193 173022',\n",
       " '34382 17808 1966 184 67 14802 1987 947 59 6449 42915 3663 628 1995 5252 46 1614 403 4878 218 2979 713 1108 3367 3816 173022',\n",
       " '1560 2832 173020 2418 925 708 24726 120 525 65 1630 1984 8880 8 2024 5994 25182 52 266 5540 5010 10209 1573 130 18430 173022',\n",
       " '12500 26813 1692 664 8573 173020 1716 218 2384 2506 732 3150 6621 8693 4189 1148 1692 591 16553 5609 18455 1245 93 1508 16872 173022',\n",
       " '10662 21657 785 6460 12890 543 122965 165 34906 10662 40 187 1222 12034 3108 413 59 3536 590 3617 413 59 7965 853 10964 173022',\n",
       " '5917 5405 16106 10264 566 1913 1110 821 26994 4125 309 1097 168 3539 50996 403 693 5252 21867 53437 9594 5464 14036 6706 26994 173022',\n",
       " '1574 76 7870 4871 2587 2877 7663 4320 18901 31336 3942 4921 13516 8409 1492 3150 2005 46 2869 220 4160 5397 76 2025 2247 173022',\n",
       " '189 267 16139 219 3269 304 9645 1191 44 1890 1962 3094 618 274 2921 592 5212 1055 8139 283 348 130270 349 173020 1493 173022',\n",
       " '21037 2926 1716 9782 114615 533 3420 4969 220 2348 14847 1716 852 10023 1494 1626 583 1222 3132 149 114615 1784 2165 105 2399 173022',\n",
       " '6574 341 1991 4898 15386 8132 8133 3043 49159 1975 5719 916 46928 1654 916 7097 190 182 916 6405 1109 29837 11134 16875 49160 173022',\n",
       " '2991 3180 1741 1118 8478 25173 2653 8477 272 589 154 1438 987 1443 1369 583 937 1542 4784 1639 52052 10169 2384 6153 2277 173022',\n",
       " '37138 5848 3636 184 6084 571 2925 48894 30503 11643 571 935 34 12602 1452 19118 81412 86 329 266 996 165 3635 3245 7623 173022',\n",
       " '134523 488 4338 33801 384 5074 2832 2587 486 7640 304 508 10334 153 10335 10336 903 10337 173020 134523 488 4338 33801 86 11940 173022',\n",
       " '14097 8 1751 3956 4855 88649 14506 12634 13357 17064 2570 12634 13357 130696 3938 12634 13179 130697 428 130698 130699 53510 1284 130700 86 173022',\n",
       " '19580 5562 19835 5045 5967 1973 1616 299 2417 526 7159 4796 19835 5182 6732 2890 3807 2422 395 1135 1137 3170 9610 664 3784 173022',\n",
       " '39761 617 6903 119 3062 13520 2548 159 2306 2619 1443 7669 654 39121 4765 375 2429 160 633 594 595 4376 2584 3350 3696 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '2833 2795 1433 774 219 2360 979 155 266 8177 2844 25851 14511 3772 13797 305 153 2833 774 219 2844 3386 1865 120 2196 173022',\n",
       " '4 104 47 30823 47978 2165 515 444 565 1884 9436 121284 130814 5355 1401 536 969 3 3 969 536 969 1736 1615 783 173022',\n",
       " '184 10325 8917 8748 13402 87707 2637 2878 1056 6707 2375 6593 184 3687 34822 27488 27488 11597 857 15826 3511 8917 416 73414 349 173022',\n",
       " '41696 4111 6734 178 3328 304 170925 2170 4796 6327 281 1574 317 1401 25225 3321 19816 3200 671 79 6777 4893 1137 953 26453 173022',\n",
       " '47717 16040 193 194 8125 7857 14950 916 7857 14950 3385 6061 169 135486 17808 369 6867 2804 1907 132162 17808 916 1968 274 8125 173022',\n",
       " '3411 7419 526 1543 5732 5733 17449 17450 24413 406 937 1542 8372 2551 3207 3791 13490 4956 1549 1044 11559 160 3152 7611 510 173022',\n",
       " '814 23343 3341 8164 2043 21815 4951 3728 1573 1442 634 2844 3070 413 1575 0 18224 1573 2298 4607 634 2844 16379 15030 443 173022',\n",
       " '218 5987 232 2136 1711 232 2136 2493 232 2136 5987 5987 7208 5885 1711 5987 13888 19478 5987 22251 5506 23272 681 19802 683 173022',\n",
       " '76 421 1614 6649 486 304 5275 3198 3278 1013 6879 13792 7847 1328 1497 486 488 304 510 1965 289 3198 5275 5731 2925 173022',\n",
       " '429 1273 832 2010 5170 1293 1346 4846 2665 3723 444 2490 236 1724 5171 46 1614 76 1407 3010 5170 3034 368 201 202 173022',\n",
       " '6574 341 1991 4898 15386 8132 8133 3043 49159 1975 5719 916 46928 1654 916 7097 190 182 916 6405 1109 29837 11134 16875 49160 173022',\n",
       " '4980 14438 532 20674 14439 42536 1265 5588 5604 1325 3269 3565 29081 1976 5673 36469 17300 3767 416 1485 3269 12077 281 1443 3277 173022',\n",
       " '6574 341 1991 4898 15386 8132 8133 3043 49159 1975 5719 916 46928 1654 916 7097 190 182 916 6405 1109 29837 11134 16875 49160 173022',\n",
       " '3828 1268 11005 1968 6850 8573 40 5604 1325 5636 1240 1630 26540 451 29294 8573 71762 10857 1785 46 5604 1325 40 959 36652 173022',\n",
       " '1485 281 283 3237 11667 521 11683 13931 5027 1957 5620 4340 31177 1432 33607 4340 31177 3772 490 1862 95001 31177 3772 95002 95001 173022',\n",
       " '173020 349 4178 12019 20852 14328 9555 3647 18059 10784 1401 911 6825 6302 1273 4390 201 1537 12383 346 19242 911 40444 6133 762 173022',\n",
       " '194 18528 8305 71 595 4653 1785 19227 486 2503 20 8624 486 376 557 1800 2222 486 2725 1187 595 580 4376 7469 12584 173022',\n",
       " '5922 2949 1518 1655 17076 342 562 403 5909 2196 5878 316 5342 413 3741 529 1577 3100 4498 285 9771 4111 17076 2384 141643 173022',\n",
       " '2315 2650 2407 2408 11399 520 38798 926 305 447 788 47 18133 21794 9490 2586 1800 3286 762 52218 7580 3170 7348 926 328 173022',\n",
       " '7479 1443 134544 4004 57 9119 4039 3504 503 304 7479 25381 2130 41680 5830 12558 7896 7508 3987 5587 17076 2467 10827 8943 53423 173022',\n",
       " '13481 55222 557 5029 2368 1984 2848 684 1056 13481 2318 10737 6749 14413 2341 13935 14413 2341 812 3249 18691 6761 23889 34181 46 173022',\n",
       " '11006 28609 15021 11019 831 12004 2656 521 8881 139773 2208 2124 4513 416 1369 55 281 414 363 2043 2297 1485 13750 3374 416 173022',\n",
       " '173020 4887 54 193 5821 12991 10955 2165 1770 6100 4505 715 2115 88 444 51966 2198 60542 13069 89 2924 1238 58 9468 9468 173022',\n",
       " '7656 1800 2222 486 304 3415 2130 3537 3538 45 7127 8712 1389 13098 271 7686 3810 86 6718 20839 6959 2759 70923 57914 368 173022',\n",
       " '3338 4373 713 674 52031 2289 69 966 2169 1436 14299 8663 7619 140 3338 3502 3814 3483 725 4189 9887 506 628 967 2909 173022',\n",
       " '634 1240 3303 145 9133 4278 3947 3304 421 628 654 3460 3818 4278 3947 2190 193 83887 7097 1434 3 6233 4373 685 702 173022',\n",
       " '3425 5569 14969 8573 363 160 3269 2523 5991 1400 7648 5252 3511 8692 1076 618 683 330 76 520 164 494 1697 304 416 173022',\n",
       " '218 5987 232 2136 1711 232 2136 2493 232 2136 5987 5987 7208 5885 1711 5987 13888 19478 5987 22251 5506 23272 681 19802 683 173022',\n",
       " '3425 1169 2933 304 2248 5255 4385 925 634 85785 3675 11839 220 4330 3425 1169 32728 2341 8546 5337 1510 498 11003 2720 50951 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '2199 5351 6965 455 11946 9119 105 1785 2330 3606 5332 84939 13396 6804 28984 13882 88 64991 1313 1222 76 47091 642 1344 4207 173022',\n",
       " '47733 267 2248 517 1042 17793 108665 59040 11601 10169 8899 958 7157 3034 368 5617 3034 368 3076 664 1039 8903 2030 1584 3445 173022',\n",
       " '1205 33712 38146 153692 1124 8262 1707 364 304 52 1995 16773 3809 2046 1124 3297 154 2761 304 677 33712 38146 47 31878 4145 173022',\n",
       " '12244 232 2136 4850 45 15296 13865 6386 6338 7200 4850 45 24 925 789 6923 120 1826 15296 2198 37993 18686 628 38981 1117 173022',\n",
       " '128486 3141 6061 2041 23600 2222 2640 1113 73 23601 184 2933 486 2793 304 1438 23600 416 1288 128486 10343 416 1369 3269 321 173022',\n",
       " '674 515 3610 6481 5897 219 14530 6903 35211 10537 132781 369 152032 5819 1311 219 5901 7221 1697 490 4378 1329 3013 45864 369 173022',\n",
       " '3712 173020 2534 160 23 3694 287 4313 4315 22938 8880 1239 210 1733 32311 141 1856 413 1648 917 1369 583 321 4413 5020 173022',\n",
       " '160 8936 1072 4763 7123 6888 12100 65551 147845 72410 31845 6526 2354 530 2351 4088 29803 4660 43038 1993 8899 17432 3488 6409 16300 173022',\n",
       " '14486 316 1200 1889 90 2517 882 4279 3816 3810 11641 4279 90 23 272 3142 324 1315 1925 28268 1469 86 193 289 341 173022',\n",
       " '3643 2130 19966 19967 46 1800 3925 2241 94 2423 1984 6613 1113 3456 2500 10264 566 3420 10394 4875 46 1060 57 16017 935 173022',\n",
       " '9381 126900 5922 1516 6302 3845 1527 675 1377 2733 3442 41 6203 70158 2006 1380 43791 3798 40752 16879 7187 7950 29304 23575 416 173022',\n",
       " '4358 155 683 628 634 8326 1333 4784 16223 341 5330 173020 4589 8149 4798 287 2451 108451 3628 349 12632 86 3944 173020 11519 173022',\n",
       " '1113 2783 3 7331 9757 4921 316 11946 4875 11660 1492 5050 610 13989 102853 5038 1794 5055 3712 835 455 2780 610 2169 72 173022',\n",
       " '2723 15848 4026 13406 304 674 13414 173020 13406 5562 9204 316 173020 13406 8708 4312 451 2517 13406 16673 3816 3810 7920 1076 316 173022',\n",
       " '17073 1650 3393 1136 4653 1997 5221 2673 319 1692 3791 3185 62290 1898 5976 2467 490 2911 2347 7103 13182 1898 5976 321 1615 173022',\n",
       " '486 12934 304 789 1034 2320 428 11129 674 5399 6574 1784 6540 3418 11129 1897 100 1238 11128 1784 10189 5424 3387 1369 6574 173022',\n",
       " '6234 3628 4834 4226 1913 2542 3992 53 90 6179 9990 3829 51 1610 8913 19580 1664 17235 34753 1261 15085 3628 21721 21722 4834 173022',\n",
       " '5973 2653 12898 12899 3058 4966 821 13635 1785 14937 1573 6032 7703 6110 76 24014 113939 34703 2160 3034 1491 7949 4324 2005 46 173022',\n",
       " '18808 1232 7381 2491 12269 81099 839 2300 285 1815 3716 7953 3466 1080 86 759 5715 6551 19242 346 6005 6551 6133 164 201 173022',\n",
       " '16508 2224 851 1408 1400 666 2761 413 59 6085 20125 16282 5874 5817 16508 628 52 1385 1076 329 4970 682 1469 16508 2224 173022',\n",
       " '2107 18875 5829 40623 15330 56987 1860 165 173020 2741 37312 959 618 506 1762 58025 58026 56987 1365 12012 173020 642 520 11449 8444 173022',\n",
       " '73 8473 8221 4533 3459 50927 104903 11794 35826 664 8621 5904 1984 3433 14921 1104 252 19865 11753 557 4592 173020 3067 160 633 173022',\n",
       " '8622 15648 17464 6734 178 708 304 8644 7061 316 1200 1189 27852 1740 6780 88753 3558 276 1119 686 2795 4796 1189 27852 1315 173022',\n",
       " '833 59 870 13701 6460 202 514 2926 7620 3100 543 287 759 4878 17134 403 47016 22 3984 50905 5661 5682 870 13701 1968 173022',\n",
       " '1875 40738 4207 65 274 3526 8327 2422 3804 1285 11639 3148 59781 16983 59781 5166 19446 11240 1285 1445 2323 3148 3348 4448 59781 173022',\n",
       " '6574 341 1991 4898 15386 8132 8133 3043 49159 1975 5719 916 46928 1654 916 7097 190 182 916 6405 1109 29837 11134 16875 49160 173022',\n",
       " '999 12984 307 12984 1358 12455 9872 5706 676 5252 160 11619 4141 1907 999 1492 9521 2021 10715 5306 266 184 3892 2306 2297 173022',\n",
       " '160 633 364 708 19961 3343 5731 939 22980 22981 14 127 5275 1633 10927 16675 1614 1464 1499 939 13501 8913 347 6803 4875 173022',\n",
       " '1300 2722 1344 7848 1598 2229 1709 759 3242 495 19042 3966 93513 58946 1104 579 1741 10883 29847 1741 232 40542 13806 2782 119308 173022',\n",
       " '415 160 2983 4464 160 1472 20035 161 2164 2925 543 4531 27560 2619 7243 2637 3835 591 1465 2034 1274 581 161 2164 416 173022',\n",
       " '12087 7463 6734 178 790 304 3425 3426 1988 5493 1989 2422 304 219 250 3269 3020 4568 5405 525 1692 1988 5493 1989 2422 173022',\n",
       " '41173 7468 42632 1672 1965 3425 5569 8692 2538 81409 3815 4341 882 21581 1136 32934 1245 3101 2136 211 1620 319 10625 189 583 173022',\n",
       " '2758 12872 368 5220 12826 3442 52 3363 1330 1171 532 156166 301 773 14476 611 1361 14278 498 492 1295 2782 94 1528 1289 173022',\n",
       " '753 503 180 121 3636 1879 505 16753 1280 13078 4918 2908 1445 81477 18440 1885 7 3326 9270 3326 506 97293 12700 52 155 173022',\n",
       " '365 7620 304 42178 517 1295 4403 10269 319 674 446 1295 301 42179 21976 3363 1469 207 521 908 725 7159 5020 86 2098 173022',\n",
       " '1237 13525 6654 10298 11430 12328 521 22842 20457 3425 3426 418 13525 6654 10298 11430 12328 521 22842 20457 3425 3426 15063 341 1154 173022',\n",
       " '17999 1490 5477 10955 3044 3443 250 5405 11794 3369 1715 12653 21608 2285 7663 250 50576 3044 7957 6777 416 3369 17999 3044 10955 173022',\n",
       " '5987 12445 34956 9555 634 10295 1692 106 1169 319 65 590 12950 7663 1048 760 856 1690 1762 9561 3784 3852 570 2155 4721 173022',\n",
       " '40738 5819 14104 78649 10517 3615 132247 486 2141 304 789 708 8754 683 2744 78649 10517 2739 790 3617 193 1553 1285 170 995 173022',\n",
       " '12087 7463 6734 178 2793 304 3425 3426 8997 6843 93454 2422 304 219 250 3269 3020 4568 5405 525 1692 8997 6843 93454 2422 173022',\n",
       " '1058 7638 607 4888 4592 723 320 267 3776 2205 3137 2925 25448 9953 11551 20076 1058 3662 101729 173020 977 505 173020 57325 200 173022',\n",
       " '1553 2016 9111 2139 35211 4197 118827 1270 1925 3963 153 10336 430 6225 6322 3 120 2037 3247 16592 508 23228 428 5020 1205 173022',\n",
       " '7260 9645 8936 4193 3741 1646 49383 92103 2096 5268 20 5078 1443 23694 23695 160 8936 23696 773 21 8448 2096 23697 1609 23698 173022',\n",
       " '173020 103201 2157 212 628 52 1720 7355 9556 94 974 16062 1408 465 17937 2223 15183 9749 683 634 3855 495 1800 321 1615 173022',\n",
       " '757 4376 3 1913 3341 4376 1623 2297 4384 1711 20215 46 7194 5987 4384 2302 118 128711 9950 12206 513 2856 16793 4384 554 173022',\n",
       " '13245 13045 1926 164 1443 32950 6981 1090 1171 44784 14282 951 1609 18502 1989 4446 13245 13045 8149 9125 558 521 57852 5606 4784 173022',\n",
       " '425 3784 1784 5554 205 1132 93 1389 2979 184 3033 1110 416 40406 12077 281 1968 5165 45353 970 3627 93 8854 1389 4868 173022',\n",
       " '25043 1693 671 2136 18651 96481 1693 42016 3852 5688 8965 1642 10675 4592 1650 11094 6749 48618 8899 42236 17026 24878 3150 1313 8621 173022',\n",
       " '5608 12483 46768 486 974 3410 3411 7419 6558 6553 4586 1767 4540 2030 2915 11469 2640 1303 44386 173020 8621 2933 1767 1692 105200 173022',\n",
       " '10203 12872 17333 818 1231 1894 6531 4338 2021 22484 15494 173020 61812 5877 1504 790 2262 6678 6903 119513 6803 582 3232 1174 173020 173022',\n",
       " '365 304 32370 368 369 370 96 1977 1273 363 1978 1979 1924 1809 1925 1476 8339 521 289 2401 9867 32370 368 908 725 173022',\n",
       " '3504 155 304 43258 194 173020 5453 1746 17484 328 12751 173020 3504 2761 304 47 8554 5288 5288 5288 5288 5288 47 1369 14446 173022',\n",
       " '160 3269 302 193 4140 3436 1285 4013 6635 851 3322 2497 79 56596 7951 16 5782 2425 10971 1174 2435 43565 363 173020 193 173022',\n",
       " '37072 160 590 1412 4315 851 590 1412 1763 560 1818 25024 2799 885 12826 9744 1325 40973 8333 857 766 8835 321 47 7228 173022',\n",
       " '2492 1542 5213 1553 2030 341 4784 5038 4533 521 83859 3121 487 34 202 39760 6760 11667 13161 2838 173020 4464 934 7683 3814 173022',\n",
       " '3297 7848 76 444 595 242 26149 9210 54524 484 1641 1443 99 4763 3600 1158 2751 15942 1889 189 1211 197 595 9512 99 173022',\n",
       " '6137 11326 15462 1308 4609 1331 2188 37312 350 147610 6137 15503 15462 1284 8177 1331 2188 5615 2856 208 2792 13993 198 15462 1308 173022',\n",
       " '3241 1443 1973 88 1918 10327 6593 2422 18544 2382 1787 2534 451 3610 2382 18236 10480 4197 13134 14925 1901 5486 515 723 1285 173022',\n",
       " '1560 2832 173020 12087 6613 925 179 24726 35081 61171 38 32996 6613 4980 1560 54709 2061 1785 513 821 1904 2856 1747 22552 9319 173022',\n",
       " '5574 16271 23552 3753 11620 2092 497 5342 316 307 4687 212 2734 898 592 155 592 683 592 618 2092 149011 454 32468 1800 173022',\n",
       " '144410 368 916 341 201 365 144410 368 916 6841 8040 1918 144410 368 916 370 10257 521 1925 4846 3297 8156 144410 368 916 173022',\n",
       " '173020 349 4178 12019 20852 14328 9555 13099 18059 10784 1401 911 6825 6302 1273 4390 201 1537 12383 346 19242 911 40444 6133 762 173022',\n",
       " '6959 16770 150630 1191 59880 18352 14701 575 12619 7025 7706 76794 100 6707 5707 18620 12182 27731 12423 2924 3719 510 16770 150630 20225 173022',\n",
       " '2128 698 6024 1708 30525 92105 7575 44472 698 7689 35538 1708 418 698 1778 779 27745 22257 58 4001 6961 1434 3 439 30525 173022',\n",
       " '20 178 52 1096 8520 13669 3454 4560 13669 295 2010 13669 8520 4817 3502 21222 5850 2165 569 2925 2130 2925 10932 22006 3513 173022',\n",
       " '1097 775 666 90 1857 579 439 1697 10994 1331 2534 9717 3074 87 2255 439 1375 49 6706 821 548 3359 6656 59 775 173022',\n",
       " '537 36078 2248 2597 9362 21150 3540 583 2597 41 4256 10838 13162 888 2091 3905 2597 3758 1408 1904 1708 1303 7663 6749 321 173022',\n",
       " '3618 18325 2297 4218 3242 4646 46 1785 57 4218 20721 6327 16730 32122 2191 10016 11915 14310 12558 173020 5858 4705 708 925 3328 173022',\n",
       " '1437 69745 19170 19194 94 7700 3905 10635 1298 200 46825 23889 1273 4602 2140 19170 19194 1966 4169 510 3018 1238 2937 642 1437 173022',\n",
       " '41649 4288 1315 4932 13418 9626 4308 2517 9019 6596 2242 18598 6171 6596 6600 1338 14404 188 4308 1394 283 190 7909 3698 5562 173022',\n",
       " '746 2584 19389 19390 1136 1095 2409 23 4168 7 25204 2584 2608 2609 3098 536 154 491 1574 202 3484 1904 439 1479 1096 173022',\n",
       " '2771 506 628 6951 634 683 155 618 628 3673 506 628 506 2940 9137 2554 3673 3892 7327 762 4189 3673 1226 1698 3673 173022',\n",
       " '90 630 194 1536 3828 3626 1614 19679 18649 814 4798 3420 2588 47 10682 252 2446 3074 1090 152 5234 7604 1980 10739 1697 173022',\n",
       " '12087 7463 6734 178 2793 304 3425 3426 267 52590 5934 2422 304 219 250 3269 3020 4568 5405 525 1692 267 52590 5934 2422 173022',\n",
       " '153 536 595 384 350 71 4932 200 1918 634 42466 4000 13074 2464 1598 2375 4000 5994 444 120 119506 18886 120 5854 9730 173022',\n",
       " '416 3625 3554 24446 2196 2708 15837 612 1485 326 442 6723 70829 4837 7039 4875 351 348 173020 349 1328 3349 3554 24446 15837 173022',\n",
       " '2694 58067 21675 396 1094 17197 49 23 218 965 28271 755 756 7026 444 14797 45528 72511 409 1757 28280 58067 21675 642 490 173022',\n",
       " '7075 1076 65 66 3410 4513 2043 1785 570 6042 6560 11986 120 17671 35139 3919 416 6558 6553 6561 4513 86 316 2310 86 173022',\n",
       " '770 497 6941 2273 6018 6843 21600 2659 3404 679 11397 8864 15105 5718 7093 1443 10857 173020 5831 674 934 2067 6539 497 6018 173022',\n",
       " '51633 31082 1582 160 7746 1800 40 4047 8858 845 3360 959 5631 17398 580 5759 440 2521 27545 2355 9752 3011 928 3701 5826 173022',\n",
       " '5375 173020 2136 1169 8936 252 2130 5902 11865 591 54891 173020 21019 65 41376 702 52003 43361 143 10724 2224 46 2587 2714 109517 173022',\n",
       " '2302 1934 1918 532 5632 21114 13476 33316 15100 21150 4355 1274 23968 3905 4167 20389 12134 3081 1472 3503 39173 7362 173020 2838 6924 173022',\n",
       " '2761 413 59 20710 156920 5841 45 5759 440 2109 1984 1874 10237 1136 2205 10257 6702 1355 19370 557 5060 160 3261 2619 2221 173022',\n",
       " '2691 489 38015 10955 3957 1831 3804 1896 52 202 14040 1775 232 2369 2597 713 3687 68 2187 232 2136 1091 2533 19506 1091 173022',\n",
       " '20228 12973 17464 6734 178 179 304 8471 1072 219 13131 304 25989 219 8662 8687 1623 3971 219 1370 153 3814 5213 250 3269 173022',\n",
       " '9285 1866 193 2723 8057 7780 94 1637 4133 25340 12829 26387 2723 5798 94 7562 3932 7695 2534 17182 101570 8405 591 40215 6225 173022',\n",
       " '506 506 506 55 1438 351 348 76357 349 583 1443 2492 11065 304 178 710 3618 1136 4332 1855 1273 2502 11817 2344 173020 173022',\n",
       " '7329 34 9350 120 8026 6824 4769 58364 2370 151 173020 50102 3698 2020 1333 1157 7481 4244 9014 120 9350 917 1369 911 190 173022',\n",
       " '10660 926 4533 535 2458 683 618 266 5270 10727 12072 71 6778 413 3852 73 824 4007 1918 3185 4189 363 178 180 683 173022',\n",
       " '2948 8622 3139 1650 173020 2000 3285 12746 5010 612 160 39668 5018 2136 6548 173020 232 2136 148307 1097 612 160 308 39668 291 173022',\n",
       " '66432 1491 35379 66431 66432 58 98008 532 49580 1794 1499 939 13501 8913 347 486 628 158 10997 3920 1492 41566 26916 94524 1492 173022',\n",
       " '2043 11554 1692 1693 2470 519 7870 3852 689 2470 34032 19726 54489 2341 936 10117 5509 54490 1222 2043 54490 46 19726 519 1747 173022',\n",
       " '120 666 7870 15154 3852 11054 9024 34 2794 13667 2848 28330 139495 26877 85296 959 11054 13667 2848 7182 335 14506 85296 28330 139495 173022',\n",
       " '11032 4705 180 304 11032 510 970 4197 384 3286 113 18894 5198 1138 1489 75 926 113 1285 4658 1442 1614 925 503 683 173022',\n",
       " '2426 7651 8 316 440 683 7952 7765 17751 160 689 4145 6691 13166 583 4908 4165 76 5644 18601 2574 664 19936 17544 23063 173022',\n",
       " '2096 1781 7920 5732 845 7921 46 723 36445 2042 9763 1668 24680 1671 6449 5732 1216 1542 9701 16776 418 1434 3 21178 1443 173022',\n",
       " '1840 11399 9654 200 6519 4531 560 821 2165 157 29851 1408 210 11738 3687 1427 7185 2619 15670 15670 58 1577 14711 58 2034 173022',\n",
       " '2130 3505 20701 1508 11736 2024 361 1995 3366 3217 2883 274 1505 1679 304 814 1642 173 3217 4058 3749 3011 1542 2022 46 173022',\n",
       " '3955 18457 7309 7 18057 472 93328 4996 76 1314 1408 1762 2779 2479 1472 1941 3732 472 1575 4996 3955 18457 7412 2582 1195 173022',\n",
       " '6584 5000 6059 1785 5210 5420 499 2224 3918 5046 1461 1073 5366 1635 7776 3732 1966 2092 19488 591 2786 956 5000 21212 416 173022',\n",
       " '421 2221 3852 6877 486 304 5922 925 2793 6678 1056 6707 5909 71 5878 316 5342 8552 5877 3448 21858 59190 19748 46 3852 173022',\n",
       " '2225 6434 2921 6358 1973 5568 13214 1117 6660 1763 2554 2235 2225 2225 5568 1973 3602 1904 2553 713 222 955 2098 707 1978 173022',\n",
       " '959 3370 732 2501 16648 499 2341 151 2771 2269 18930 2084 788 17533 1720 1469 17595 1443 5987 1650 39268 13172 52662 14219 683 173022',\n",
       " '76 42491 3900 1546 396 7560 1697 2078 13149 9413 1169 2534 1169 218 87 5471 3932 38644 520 444 557 89 439 132 2680 173022',\n",
       " '72740 72741 1013 1014 6678 7491 3610 3922 1785 6669 17958 267 3033 9009 24395 3301 3506 3162 6499 9885 2925 7612 32351 3301 5841 173022',\n",
       " '1485 281 283 3237 11667 521 11683 13931 5027 1957 5620 4340 31177 1432 33607 4340 31177 3772 490 1862 95001 31177 3772 95002 95001 173022',\n",
       " '14308 34187 775 5854 173020 153 55 2224 1097 6076 3082 19 486 683 173020 3956 775 2109 2865 12864 1543 4922 469 1391 58579 173022',\n",
       " '5973 6678 5972 5975 18431 3425 3426 66733 1551 3065 786 347 15431 316 26082 1260 3843 10278 1697 31982 5782 10264 566 6678 1984 173022',\n",
       " '5887 3447 21872 9004 2650 11611 11611 11601 3425 3426 21872 95 23 1856 413 1210 1984 15992 5554 55538 24006 32785 9004 1615 783 173022',\n",
       " '6249 6327 5976 1622 2808 9036 2165 1096 31805 144639 2970 8553 31805 28370 1787 565 3263 1323 416 1874 8087 351 16186 16187 173020 173022',\n",
       " '55 2318 8615 1117 3780 14212 6242 1150 1117 10759 6358 1671 6242 1150 47 2298 9631 14212 339 1744 1150 1641 3758 4751 120 173022',\n",
       " '316 33533 753 693 304 628 179 988 2320 9553 618 8578 1499 939 27902 27903 6822 25317 3662 9951 1904 2232 6456 10282 416 173022',\n",
       " '9142 173020 905 10577 4073 20567 17088 3708 31741 41 2418 31827 9181 20472 12330 4284 9645 676 901 1298 10065 6723 58 8821 881 173022',\n",
       " '8100 6642 58 813 11702 40 898 749 13785 1778 15714 671 27837 442 7784 25582 34 16 815 2013 47 1369 583 917 1784 173022',\n",
       " '3301 16619 324 53045 3390 1545 173020 502 8506 4140 601 219 1285 488 2841 153 2165 1621 6949 965 7688 1993 2922 2482 3304 173022',\n",
       " '2190 1443 8302 283 6571 1443 8302 9868 155 34156 9111 1925 8910 13237 1060 1060 630 1443 8302 1285 52 16443 2366 506 16443 173022',\n",
       " '34 7947 4572 76 571 157 11746 20832 36037 3 713 858 969 1062 1135 2665 444 98579 173020 3289 2534 1581 1172 8398 416 173022',\n",
       " '472 38527 13900 6841 56127 22408 4848 10065 191 488 13803 781 1590 789 304 3771 7124 22408 2005 1097 931 4801 1692 3219 17563 173022',\n",
       " '7623 7491 3610 3542 3104 3852 996 2005 46 219 40415 2503 24396 3301 41475 12040 1984 4189 7610 88 1794 173 2925 7626 2289 173022',\n",
       " '160 633 3609 575 15207 13567 3349 2160 179 202 189 4641 35211 1962 7157 1082 21443 9322 33755 1336 1285 1756 672 74132 349 173022',\n",
       " '86775 442 120 96 86775 113 1818 10116 1400 1857 571 169 2771 24631 759 3696 1227 560 105 113 1598 3696 592 2744 59 173022',\n",
       " '396 3 767 2744 2554 94 218 2136 570 272 158 321 671 2046 41812 23575 917 2053 4589 396 6567 484 26189 32285 5347 173022',\n",
       " '1661 31906 173020 46 2130 28096 9739 3224 6449 10210 22233 1094 105 1268 7923 3175 1551 10977 10300 13754 418 1434 3 21178 1443 173022',\n",
       " '759 33615 4251 3592 384 18791 628 708 2320 508 10334 153 10335 10336 903 10337 4251 3592 148 11044 759 33615 845 4426 7185 173022',\n",
       " '69054 2912 415 1997 452 693 469 10222 46 403 589 6553 1982 814 418 1434 3 9153 28757 416 2910 6553 3269 7140 469 173022',\n",
       " '12724 12728 3338 6750 46 273 3241 328 3458 2542 6750 1432 3306 3141 3338 19459 274 12724 12728 3338 6750 19758 32164 334 38 173022',\n",
       " '1485 281 283 3237 11667 521 11683 13931 5027 1957 5620 4340 31177 1432 33607 4340 31177 3772 490 1862 95001 31177 3772 95002 95001 173022',\n",
       " '5934 3338 16040 6867 3686 7918 12724 7159 4533 2188 6777 12724 328 14278 12724 1968 3241 1647 4576 4326 418 1434 3 21178 1443 173022',\n",
       " '26092 578 1436 591 40 319 1137 153 55 3398 629 3542 8085 1273 58 1273 390 47 4551 5789 7822 15299 5954 180 4428 173022',\n",
       " '3363 495 3619 173020 369 4562 818 27632 4374 304 451 1756 13014 4801 2098 553 89 1607 1492 671 3636 685 818 27632 818 173022',\n",
       " '1577 5535 2105 12341 11387 42974 418 4560 1688 718 16979 37632 6434 1434 3 316 90 4 6242 594 4166 1395 11044 6735 4178 173022',\n",
       " '9650 9646 11395 1509 2489 8109 9697 2745 56 46 1614 26621 6972 2304 1984 2794 9665 9654 46 789 266 3947 93 7954 2795 173022',\n",
       " '285 2279 18244 3791 190 157 557 2318 200 190 931 3363 2861 6064 840 841 566 821 281 5662 347 289 1369 7097 281 173022',\n",
       " '14331 153155 2224 14580 403 1248 1106 557 857 9816 5720 17483 1057 746 2653 15031 4255 1076 444 680 403 1097 2665 564 410 173022',\n",
       " '4887 1096 583 1104 721 34 3631 699 165 702 17528 1138 20490 287 491 1063 2372 7659 21894 702 316 403 439 1369 55 173022',\n",
       " '17364 6531 31047 10080 8292 15198 4586 2649 2704 112635 14322 1685 138308 1066 17364 3218 316 2704 840 1217 1866 533 3543 31047 10080 173022',\n",
       " '4299 4300 4367 335 9374 6327 4299 4300 9492 1798 30943 173020 16199 335 9374 4111 4421 6860 12202 28975 6327 797 148416 1874 429 173022',\n",
       " '91692 91693 15105 82990 92103 2042 20081 1499 939 13501 8913 63019 6449 2465 1210 1534 7851 3390 11321 3306 33286 8899 683 1921 514 173022',\n",
       " '1549 361 4101 3461 56807 1703 5346 27420 4320 116281 23611 10459 37032 3915 3128 4101 2537 7929 47668 2441 250 510 15769 454 24306 173022',\n",
       " '71674 23334 970 683 676 10357 1755 970 1285 27121 15669 931 634 676 418 71674 23334 634 676 10357 1434 3 69869 15669 862 173022',\n",
       " '1485 1486 760 488 1328 3349 1488 347 351 348 760 7939 349 1328 3349 9039 8970 2496 35250 2300 132 2173 16495 173020 19281 173022',\n",
       " '1671 3493 2827 160 8705 7136 4324 3922 536 595 3804 1896 1240 2744 2962 2245 219 1237 7635 2875 1553 8685 3241 219 1910 173022',\n",
       " '2925 4303 10212 2546 5275 3784 5333 996 5875 2034 5275 4891 274 1235 1491 3387 2007 5718 16567 674 554 537 7623 3784 2422 173022',\n",
       " '396 3 591 3892 683 2555 120 2426 5848 3247 2248 302 8204 7853 2313 173020 120 3150 715 387 3934 2417 715 1698 10845 173022',\n",
       " '753 179 180 126 3493 2427 6593 8057 18 3830 7039 17997 1611 1434 750 7867 566 10180 21866 2427 2428 1472 4597 4324 8864 173022',\n",
       " '10046 34 13614 5035 120 2111 193 956 1461 9020 34 6794 2788 78859 33368 5056 1692 2224 46 3306 43830 1443 683 1921 514 173022',\n",
       " '25043 18283 3139 1650 5802 5420 7512 1245 974 10917 1649 3253 974 1711 1649 38810 2096 2136 1769 5420 7512 1245 974 10917 1649 173022',\n",
       " '486 6648 304 7640 979 38099 38100 460 17569 6348 1136 1649 6569 1762 561 1814 495 33037 4374 5929 60198 17562 72291 11129 1897 173022',\n",
       " '421 590 1800 12935 486 304 10561 7448 1589 71 16097 20158 30360 15196 486 628 304 4742 12606 7867 10567 2736 510 60142 105046 173022',\n",
       " '974 413 59 2131 80323 666 1094 1657 789 202 937 5994 558 28173 6633 54319 173020 34915 23390 1442 12898 12899 5839 679 173020 173022',\n",
       " '2488 16916 173020 4464 1641 15466 2725 416 1369 55 281 414 363 369 685 1485 13750 3374 370 3383 4932 200 12442 13048 153 173022',\n",
       " '14224 123672 3852 46 17952 2431 124227 26987 1925 6076 3086 2875 5826 3516 9220 123672 20461 4906 9127 10706 2431 38130 124227 35383 76603 173022',\n",
       " '16492 634 5239 61374 5712 20 16099 3899 1752 7912 2396 6075 15030 28795 2067 316 440 9224 375 16492 126 6599 852 1844 4373 173022',\n",
       " '10159 925 505 492 7316 939 28875 69506 3422 4497 46 1614 1814 1426 2778 1630 1499 939 14254 38009 38711 148487 1731 1710 1542 173022',\n",
       " '12895 348 173020 349 3624 127962 5654 159 44411 22352 4408 4490 283 418 127962 5654 159 44411 22352 1434 3 5438 9396 1395 4403 173022',\n",
       " '10527 173020 6061 2041 23600 17232 2640 1113 73 23601 184 2933 486 503 304 1438 23600 416 1288 10527 173020 416 1369 3269 321 173022',\n",
       " '34388 8527 12624 4837 4 814 10267 1752 5051 2165 656 2650 3474 1312 2707 8646 525 59957 1464 3004 21217 2933 1137 1111 200 173022',\n",
       " '40738 5819 40739 469 1862 3615 173020 1523 5652 304 75 974 8754 683 171 469 1862 2739 2841 6931 193 1553 1285 612 13099 173022',\n",
       " '346 321 416 583 1369 10246 328 5394 6463 1174 173020 723 46 569 4440 32285 273 3 3019 4576 3147 4576 4896 67006 34189 173022',\n",
       " '21039 2784 59252 184 2523 502 5468 1346 1878 634 505 4448 2204 1004 8751 506 628 487 5300 490 2771 506 628 634 1009 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '42010 444 4201 1395 4167 1108 40872 49 858 1768 80595 1641 16758 899 14633 19701 173020 1095 881 3263 2688 1395 4167 880 8350 173022',\n",
       " '7034 1766 44170 3896 1528 10991 1274 9998 10931 403 52228 3712 1245 2493 1649 258 657 4681 46 1274 44170 1672 87 5682 682 173022',\n",
       " '2023 21018 850 30527 46495 120 1814 14557 6327 418 832 272 772 1434 3 316 90 4 6242 594 4166 1395 11044 6735 4178 173022',\n",
       " '8870 14227 2587 5791 76 28337 45209 9638 295 3091 6419 18528 2169 72 66 1692 232 956 6481 1542 3492 16 28701 970 11311 173022',\n",
       " '2344 2404 25058 2341 2948 25057 1726 723 486 488 8977 19353 2920 591 4376 25058 2341 5632 328 1527 155119 27643 4464 1615 783 173022',\n",
       " '9285 7018 1708 580 5588 6327 7667 415 204 45204 3832 347 7802 152655 220 917 1311 1961 289 173020 349 429 762 2554 769 173022',\n",
       " '1784 506 10246 2075 1755 27190 8584 3425 1675 5987 634 193 4245 5544 1815 2017 155 11806 2055 9998 4721 30979 2627 818 33155 173022',\n",
       " '25043 96481 1693 25043 2494 8705 40757 3963 1650 92583 727 40757 2260 20386 1650 4671 2240 40757 5842 3605 5885 6548 34487 1897 4671 173022',\n",
       " '759 53045 141 10466 176 11378 22897 22034 20221 22898 4145 333 1303 833 4159 20221 1747 2565 71001 22897 50853 845 130 759 173020 173022',\n",
       " '663 8354 36068 11245 767 1226 583 5756 2034 3082 591 657 7088 814 5874 3550 937 5695 2338 3521 363 7137 160 633 193 173022',\n",
       " '114894 1443 3504 2761 304 384 57242 10042 789 2583 2320 508 10334 153 10335 10336 903 10337 1417 114894 114894 1443 160 368 6327 173022',\n",
       " '12395 1435 1874 8872 1395 127 4366 22113 15333 25219 2534 1809 3268 418 17338 594 9194 16891 130081 70466 1434 3 9365 71374 685 173022',\n",
       " '1485 1486 760 488 1328 3349 1488 347 351 348 760 7939 349 1328 3349 173020 267 1775 20326 1966 671 10059 5548 173020 19281 173022',\n",
       " '5962 79723 38017 4075 173020 384 372 78495 9291 3650 634 180 2320 508 10334 153 10335 10336 903 10337 118 22701 5962 79723 173020 173022',\n",
       " '171197 1545 11517 9665 9657 32850 1747 8857 9660 1509 9646 866 14150 9665 9657 7212 236 8109 9697 93 12210 814 2745 368 2758 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '1542 2323 2538 86 75 266 953 1919 3121 153 6749 3633 3141 17384 17551 19459 274 161668 161668 22596 16040 19758 32164 334 173020 173022',\n",
       " '55 13439 1443 12794 1882 19726 3651 4834 3651 15025 221 20504 5735 196 96578 1801 4791 1395 1874 30756 1443 351 16186 16187 173020 173022',\n",
       " '6267 17988 218 904 533 132 1137 23146 12077 780 9726 674 904 6267 1489 160 911 2977 17569 5071 2012 4596 2287 586 350 173022',\n",
       " '26582 3034 6689 124048 3422 156 7 23361 717 29 6443 18224 120 11015 120 11249 23343 23365 23366 160 8936 1785 510 118399 121941 173022',\n",
       " '2199 5351 6965 455 11946 9119 105 1785 2330 3606 5332 84939 13396 6804 28984 13882 88 64991 1313 1222 76 47091 642 1344 4207 173022',\n",
       " '153 55 8859 21768 11449 4000 38874 4228 54591 970 139535 1785 154 488 1543 9194 970 571 23 1248 1609 1560 3956 19352 33703 173022',\n",
       " '7136 4189 11547 3835 413 4728 3504 13905 1289 3866 13349 1995 1132 6535 1984 937 100 2384 1450 2533 966 34 2744 321 1615 173022',\n",
       " '210 1260 36638 15916 7947 726 91708 517 219 30520 1380 21078 1740 38088 76 6042 5001 3544 5449 316 3334 1744 36638 1331 5208 173022',\n",
       " '47499 1332 8780 5722 2384 4542 23128 3776 2156 8780 1785 591 1542 111 4542 4344 13535 3154 5268 2187 94 5275 5747 3301 56700 173022',\n",
       " '6678 384 2587 486 155 304 634 2334 2320 1984 2637 3591 2522 2209 704 3070 2766 287 6449 3463 1692 378 9220 1692 1650 173022',\n",
       " '54424 12968 439 7867 95959 10 126 76 6841 654 664 25744 20463 13397 11576 39664 9037 173020 12968 3451 9595 2723 30622 726 1496 173022',\n",
       " '7479 5987 7508 9548 842 455 17443 14605 160 633 926 9548 1131 628 634 2407 2408 2977 3286 1707 1995 1800 3398 133101 1883 173022',\n",
       " '498 1985 11915 160 633 594 595 3853 590 30197 594 4384 3953 3460 95081 43586 17921 845 3845 4922 18014 29741 173020 114558 1058 173022',\n",
       " '2160 158256 1377 9676 6955 2125 4319 155 36934 159059 97865 46151 2561 21975 439 537 161 15976 47 4355 675 18370 6955 5475 173020 173022',\n",
       " '153 55 1464 1073 27190 4166 1516 7530 120 4592 2587 364 179 1073 7530 1313 488 708 753 11806 5688 173020 664 6632 1333 173022',\n",
       " '1683 169213 14757 8958 79347 8958 27037 12115 18381 384 83 5023 12233 155 693 10337 27037 12115 18381 1683 169213 14757 8958 79347 8958 173022',\n",
       " '13949 17887 77204 4980 25970 1328 1497 418 77204 2384 424 966 69408 683 4725 46 1679 1097 8896 1516 219 21310 5991 5677 7143 173022',\n",
       " '2739 113 10722 7738 3261 1344 281 318 200 32177 832 418 173020 12973 6871 3436 3303 71421 71422 1434 3 50542 148 61 372 173022',\n",
       " '6268 2386 1707 18371 106 3234 4118 710 701 126 18371 18372 505 25848 25509 1720 316 120 2323 11480 2813 2521 7606 18048 1044 173022',\n",
       " '173020 492 21 4595 4956 1514 20938 23132 7383 6146 4595 667 173020 5472 492 4740 40 1231 6175 3188 591 570 89 29639 200 173022',\n",
       " '9240 780 8835 2637 517 6290 4880 628 8021 16807 35362 1295 4880 7558 1346 10739 61833 72757 33383 34211 2391 3713 35865 29813 12973 173022',\n",
       " '2352 20899 1668 30280 25086 126309 746 1693 173020 20899 2063 2465 3873 10418 16297 2096 1693 8573 40495 14919 4428 8627 363 20899 193 173022',\n",
       " '1693 18871 2441 1883 5405 553 193 6648 316 1497 2559 2441 15373 18871 21043 1327 90 11046 5208 24038 14 41632 3079 32592 3041 173022',\n",
       " '6080 173020 77560 403 444 697 7927 3086 17902 4543 1375 2318 858 3507 1166 1094 157 3940 7663 853 602 820 8580 232 13830 173022',\n",
       " '113 1445 219 3628 1083 773 3658 1283 12101 486 1044 304 489 7640 2320 153 55 430 429 4331 41653 23127 8086 374 18022 173022',\n",
       " '153 504 508 3521 3897 12392 12392 12038 30527 1373 30524 2183 8570 10972 30525 2078 414 4176 2708 1373 30524 683 3624 12392 30527 173022',\n",
       " '86 2341 1965 21600 5027 679 1125 10977 645 32026 956 3250 34 4455 11397 414 4996 2034 106 18779 956 772 6327 1369 3269 173022',\n",
       " '3632 2978 10679 821 8463 676 701 12905 3077 316 10679 8692 979 1597 16388 2001 4602 5214 17863 9383 921 1212 10678 5826 15928 173022',\n",
       " '55 2318 8615 1117 3780 14212 6242 1150 1117 10759 6358 1671 6242 1150 47 2298 9631 14212 339 1744 1150 1641 3758 4751 120 173022',\n",
       " '160 633 160 421 610 2627 15088 6730 1434 63 610 5529 19769 2587 449 571 1553 3410 4513 34 63 570 8536 513 37895 173022',\n",
       " '9717 23540 838 10394 771 14420 594 595 3132 1331 4651 7054 4338 31549 160 41074 9730 25576 3231 591 46517 1108 23540 628 592 173022',\n",
       " '30979 2627 25818 683 642 51438 93240 9548 2649 2733 120 173020 5342 3272 1333 592 11806 30979 2832 51438 93240 9548 2691 488 11806 173022',\n",
       " '3796 20736 176 6734 178 7640 304 8644 7061 316 1200 1189 27852 1740 6780 88753 3558 276 1119 686 2795 4796 1189 27852 1315 173022',\n",
       " '1985 32577 4517 14777 173020 173020 2691 24234 6024 34802 4193 91 9146 60476 3723 12305 12633 60476 570 4177 19840 16450 7294 57336 48382 173022',\n",
       " '1847 106 1076 52 676 490 1997 925 1044 5718 11282 11282 12407 184 93710 46 1473 4769 132016 34972 2848 173020 3953 1056 13891 173022',\n",
       " '2856 1336 486 179 304 2319 1290 701 1784 90 1092 56581 1314 153 4849 6285 153 37316 454 151921 173020 135345 75905 27558 17242 173022',\n",
       " '575 7667 3840 31889 12627 1615 2856 2694 444 14026 4376 5954 169 25078 3286 18492 1076 2787 20 7039 4428 389 1800 814 173020 173022',\n",
       " '304 82716 433 517 153 3810 1445 8138 8139 33533 9585 8167 82717 10001 219 1443 250 47 4909 328 1338 1896 683 14282 210 173022',\n",
       " '6974 22389 25765 1377 7020 8749 1652 2542 55258 1713 399 205 5560 7097 55 22389 25765 8749 1652 2230 20136 173020 370 5586 1136 173022',\n",
       " '3928 4895 6694 10257 4731 23602 9882 4934 1692 1554 8879 160 3044 4320 5641 328 2007 931 7716 3217 7075 65786 1704 3947 416 173022',\n",
       " '2055 2056 925 753 304 1481 4846 46918 1358 184 3615 108436 193 194 6441 64863 12518 1509 2489 4761 3184 959 184 2640 73 173022',\n",
       " '19172 398 557 40 15754 120 4142 969 564 2061 316 1885 1883 612 202 514 11960 7224 833 47 47973 5388 7477 105 898 173022',\n",
       " '153 55 8859 21768 11449 4000 38874 4228 54591 970 139535 1785 154 488 1543 9194 970 571 23 1248 1609 1560 3956 19352 33703 173022',\n",
       " '71 2733 2713 618 2208 2534 160 23539 50525 173020 211 4358 283 155 12176 912 1338 12800 1391 1868 469 1514 1327 626 70675 173022',\n",
       " '1630 15284 2487 83484 76533 663 51928 120 451 1368 14163 45 1997 19013 194 2546 9380 1401 165 10832 571 515 17643 7202 2801 173022',\n",
       " '770 771 6832 633 2976 7586 575 16175 5677 759 3712 7562 4 5898 151144 169 34559 11805 92560 8782 3214 220 3621 23812 1868 173022',\n",
       " '41164 486 503 99109 11855 506 589 51 65708 1515 1614 2066 65 2744 5765 6297 27403 19021 205 5065 16 416 583 6571 432 173022',\n",
       " '5381 2328 118 6283 2160 510 520 1537 16213 42047 5889 6437 4494 17140 10436 11995 43460 313 1400 160 86 30873 821 49 834 173022',\n",
       " '4602 11977 5677 8540 9205 1422 27409 2372 3502 16528 5569 13012 2028 3132 3687 6531 1742 6434 34503 6353 1260 30884 974 618 592 173022',\n",
       " '20228 12973 17464 6734 178 2793 304 6598 1072 2390 219 10237 12988 304 219 1370 153 2422 5213 3814 3148 250 3269 13131 683 173022',\n",
       " '160 223 4796 2064 9719 173020 5851 193 569 47 390 198 1400 2221 32 160 2064 9719 223 4796 5851 1434 3 125885 18204 173022',\n",
       " '1984 1421 14202 2856 2078 232 2920 219 29763 1984 1421 14202 18036 2078 232 1442 13905 1434 3239 13457 68928 351 65605 5183 173020 173022',\n",
       " '25915 753 505 304 683 1994 503 9553 618 67001 5700 86216 12968 3609 1303 34 4223 34 1257 3112 67001 3956 2856 2379 416 173022',\n",
       " '584 122823 7086 2248 1762 218 2954 47195 23867 3055 1502 1781 1714 591 1531 1532 194 571 2512 122823 10818 1494 1495 1421 2320 173022',\n",
       " '23712 562 8503 230 319 30243 1096 814 160 5405 58 30267 1708 319 84428 1329 3170 439 6739 4237 3834 30243 24410 444 16167 173022',\n",
       " '2665 2926 342 562 227 6126 689 42791 418 68550 601 2216 2907 1434 3 316 90 4 6242 594 4166 1395 11044 6735 4178 173022',\n",
       " '24918 93068 185 27414 1273 682 149 969 682 113 926 12194 416 17410 557 18785 173020 220 3565 12194 113 157 682 93068 2888 173022',\n",
       " '6551 6110 346 6005 6551 521 1443 62488 1784 216 6144 3239 1908 4503 1809 105 4503 762 3239 1908 4503 1809 105 4503 762 173022',\n",
       " '7651 5275 3198 160 8936 19370 8479 642 8222 18505 3540 31082 9572 3068 7127 20903 4181 5820 6766 1223 7146 148 9446 2301 48488 173022',\n",
       " '3583 2375 3751 429 762 1314 160 204 31603 11597 517 7521 517 7193 521 3704 4798 173020 281 5662 347 289 1369 7097 281 173022',\n",
       " '4441 424 5421 7439 892 4426 2723 17571 5553 13586 2118 9823 1274 1328 2534 161 173020 349 22168 721 582 18021 8302 19956 8302 173022',\n",
       " '9242 14461 6061 2041 23600 40403 2640 1113 73 23601 184 2933 486 790 304 1438 23600 416 1288 9242 14461 416 1369 3269 321 173022',\n",
       " '160 8936 3067 3212 193 2458 2096 10436 6598 1652 725 13765 1169 2465 8749 7097 55 3067 3212 28318 2096 10436 6598 1652 725 173022',\n",
       " '46669 144 7269 925 760 304 4905 1488 1451 1452 46669 250 4092 1609 46669 194 193 1349 35620 22200 170629 1451 3307 3184 7867 173022',\n",
       " '52530 34 1554 140 2411 1448 49957 68724 25158 51487 33370 1509 939 66464 15497 4904 1904 15298 27459 62097 1200 51487 33370 2042 1999 173022',\n",
       " '3802 3896 3791 7259 2739 4308 17469 1369 583 351 60116 4834 349 4207 22264 155 173020 19408 3609 27143 3169 7259 27849 583 1493 173022',\n",
       " '180 178 2334 18180 16569 3826 2122 2123 20091 10948 2831 146311 7420 16569 117425 12009 18068 31339 97775 1527 1113 11660 5716 4445 1650 173022',\n",
       " '120 193 3610 1735 152407 3615 152408 683 488 505 693 490 1878 4448 506 592 634 41841 8751 1878 506 592 4448 487 36836 173022',\n",
       " '105281 27951 4207 173020 2064 2219 105281 10644 4531 1844 3504 2761 304 2172 2619 57881 57881 4532 113 47290 105281 10644 13352 58521 36186 173022',\n",
       " '58 3410 17432 86 10160 157 14492 3185 3486 69 9526 8843 39290 24190 58 3410 17432 86 10160 157 14492 3185 3486 69 9526 173022',\n",
       " '173020 7061 316 16738 3767 27239 857 2386 14691 20811 9464 3263 18840 4932 9645 94 15709 18261 1369 55 346 347 348 173020 349 173022',\n",
       " '5922 6583 4169 12864 40 5198 7651 2433 8326 1499 939 13501 8913 5376 42 1313 5922 1791 170 4875 33393 749 1118 7932 173020 173022',\n",
       " '5065 2282 190 4576 3084 917 55 5066 851 190 4576 917 55 5066 4503 1170 190 4576 525 509 6571 851 2366 1339 120 173022',\n",
       " '69211 173020 1693 783 2136 4988 79 6515 5020 8780 7437 1137 953 403 14436 3033 3148 220 424 1570 9665 2958 8899 2640 15897 173022',\n",
       " '47733 267 2248 517 1042 17793 108665 59040 11601 10169 8899 958 7157 3034 368 5617 3034 368 3076 664 1039 8903 2030 1584 3445 173022',\n",
       " '2245 867 1119 15452 677 3314 845 471 2317 2092 1664 14792 6426 20604 3694 2245 845 560 578 3314 307 1593 6195 1905 20604 173022',\n",
       " '6660 11044 14077 1884 16032 171 514 1598 32052 25510 5917 443 3 1617 14511 24249 17195 9436 28913 4721 12912 4792 4141 6791 173020 173022',\n",
       " '1456 6665 236 1519 37977 3101 3651 236 3467 2425 2092 3338 16025 994 5252 28925 3301 3173 1614 3502 37977 3254 580 5588 73 173022',\n",
       " '3643 5352 34559 5194 6114 94 2409 1800 5 93 16583 16036 2872 1818 86 1323 7068 3643 5352 12246 1108 6884 855 898 3585 173022',\n",
       " '2491 168 168 595 22522 2491 7637 710 364 173020 2832 22522 3845 22522 3897 7700 2092 8492 4188 71 1610 64880 188 5343 1614 173022',\n",
       " '759 409 1630 2130 6441 2720 6442 2237 14016 13417 1408 3438 10403 1391 1516 3662 30597 2130 4953 5991 63431 363 178 503 683 173022',\n",
       " '10825 2341 3827 3852 2458 5657 6665 12139 1931 5046 5759 2884 2342 6312 13956 46 5657 526 36798 403 273 3314 4398 1491 19212 173022',\n",
       " '19837 5911 851 5980 13818 821 595 321 350 917 2937 3297 962 11884 105287 1436 702 10196 5847 4592 449 1844 821 13427 13431 173022',\n",
       " '6065 2780 1883 1369 4576 2780 951 16820 4769 3559 4926 1878 224 1636 6065 2780 951 4882 9686 4898 2780 951 20660 10737 210 173022',\n",
       " '1526 3173 2341 5587 2335 153445 14921 93 1096 5039 3968 1834 2594 40 1329 13399 5587 2287 3450 18308 173020 4256 173020 87145 842 173022',\n",
       " '1553 3503 30869 10779 1509 9796 5367 5038 580 5765 2205 3091 1406 319 5685 642 17892 15943 3536 7691 5707 416 1874 123502 1443 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '1692 2352 2000 222 3241 12746 10790 5012 3874 796 3092 5875 2005 46 1785 2744 17484 5283 2271 1650 79 3241 6717 321 1615 173022',\n",
       " '6327 762 634 634 178 2793 180 509 762 9800 22158 3852 5987 9193 3784 1711 173020 23272 2082 762 634 1438 173020 370 96 173022',\n",
       " '2875 342 2503 1336 810 54642 413 481 44274 13479 6066 1615 2867 439 19724 767 491 224 7620 6832 2587 11946 481 3905 7951 173022',\n",
       " '715 4801 18602 9240 4513 1450 2194 4778 4750 7128 7127 9002 403 746 12020 1260 54796 1984 1652 341 3138 46 5405 415 193 173022',\n",
       " '152159 1295 11085 1671 2878 1401 205 1486 152159 789 13296 925 634 3769 505 32848 413 413 1438 3521 91002 184 1815 87001 2570 173022',\n",
       " '860 29654 30979 23222 133261 2961 4602 658 19480 8780 3836 29143 2627 160 3886 30979 2587 486 155 304 53445 1751 8780 3836 27629 173022',\n",
       " '20748 105 2619 6450 674 1814 48472 1219 4790 3359 1375 3153 3956 173020 14130 682 444 3811 1180 1169 5763 1175 1763 2619 6450 173022',\n",
       " '699 5660 4352 642 113 1768 145 36401 30671 10455 7202 7690 3968 11290 13797 1239 15555 219 37593 7652 153 1708 1303 1639 3116 173022',\n",
       " '24820 120 469 1391 3643 287 166046 8055 14265 5301 42120 57583 607 120 166046 8055 14265 17712 17265 4658 12380 15670 9577 7190 2242 173022',\n",
       " '486 180 1187 69 173 5222 1372 2980 1187 69 4562 715 7127 3610 3121 1865 44 1451 281 5662 347 289 1369 7097 281 173022',\n",
       " '6574 341 1991 4898 15386 8132 8133 3043 49159 1975 5719 916 46928 1654 916 7097 190 182 916 6405 1109 29837 11134 16875 49160 173022',\n",
       " '3340 75 266 76 27286 26325 7928 7657 84827 2375 78844 62966 105635 105636 8021 2866 4079 76741 38723 46 418 1434 3 21178 1443 173022',\n",
       " '152921 38818 302 723 2587 20145 7323 5001 24047 97062 7674 4264 532 20076 18187 19260 3712 17850 14792 20237 218 152921 11178 95836 6600 173022',\n",
       " '1665 4079 90878 2831 173020 9571 2831 60039 971 8898 2949 15330 1470 976 812 939 6981 16138 13090 971 2949 73 2160 23 173020 173022',\n",
       " '5922 5562 6734 178 710 304 40036 667 2428 3120 521 126 1359 16123 2461 667 2271 45033 14996 1359 10793 9693 27240 4841 47043 173022',\n",
       " '8780 3978 9526 2422 10208 2360 22590 801 4705 789 304 9526 5562 1492 3750 53021 19828 363 3 3341 19828 132777 416 1369 55 173022',\n",
       " '173020 10827 18633 2949 5848 2262 52 612 5176 505 13201 18048 10827 15608 76795 7371 548 7180 37164 1697 18048 2649 832 536 26533 173022',\n",
       " '2224 42677 3269 3550 1614 2653 104 5874 14792 2341 33182 2793 413 59 173020 21018 42677 2933 628 1995 12766 52 500 956 1976 173022',\n",
       " '2723 4169 26514 3542 9204 19413 5063 29586 14565 5063 2534 65 19541 10125 5883 3796 10302 173020 867 173020 14470 1198 39622 31715 29586 173022',\n",
       " '2314 2248 10560 2018 925 1044 2904 1681 28 1528 5951 3483 1610 80739 589 925 75 1551 1720 1238 2368 11448 71 1542 2915 173022',\n",
       " '25043 1118 9173 128513 2061 1210 32673 444 11378 105746 1614 2714 5819 96300 5833 128514 96300 622 3019 753 2714 6972 105746 486 1044 173022',\n",
       " '34 1554 5972 5975 4481 786 1142 2042 3282 3410 2423 1614 14751 25433 933 10169 38371 75 2744 14988 7182 3113 3178 6448 3081 173022',\n",
       " '304 178 612 3607 710 2830 5536 1878 2742 11772 17551 6061 9656 1333 925 612 304 10116 2801 304 39509 13619 6061 370 96 173022',\n",
       " '12573 74442 23802 833 2637 93 2279 7055 974 1310 24668 1222 1393 4178 5834 1899 413 59 11971 2316 6623 14613 363 12034 193 173022',\n",
       " '421 2221 20 6511 486 304 2407 2408 33618 2608 90710 34310 4114 43287 173020 9378 881 3 9365 759 2844 220 3043 87 9365 173022',\n",
       " '14122 1406 1294 1451 1614 194 9658 26723 18802 4378 135407 7111 259 5580 6117 2262 2844 37160 18482 2490 8114 20354 3818 2153 7111 173022',\n",
       " '7105 24807 24587 2375 7058 446 495 1280 578 2645 3712 835 154 416 19934 45211 772 24587 674 7105 9152 452 31745 1708 91178 173022',\n",
       " '563 7562 575 33685 173020 48178 9715 1196 498 21212 785 563 7562 575 33685 173020 48178 9715 1196 498 21212 785 510 21212 785 173022',\n",
       " '105 509 2386 160 1994 634 106954 2300 3127 4839 15669 10387 2339 489 3676 4299 3127 155 1886 931 155 38 396 2136 27335 173022',\n",
       " '75 5562 317 1097 6242 11594 2318 2465 557 160 9885 452 5562 1200 3629 6242 4092 6242 4533 1874 20225 351 16186 16187 173020 173022',\n",
       " '29791 316 1443 1614 6649 486 304 11722 18370 37505 7259 182 56343 18365 1566 11327 685 15079 42604 580 4850 120 642 1206 31031 173022',\n",
       " '36106 1081 1578 9028 19323 449 218 2433 36127 3354 2495 536 2587 3398 1317 18 4004 13018 45647 2433 675 2433 12864 7029 173020 173022',\n",
       " '141 6144 40407 4740 18021 1959 3004 9693 2375 141 1463 76984 762 2375 10956 346 2046 141 10537 321 5743 16 1715 571 1750 173022',\n",
       " '51487 33370 1509 939 66464 15497 1607 1473 2969 3138 3383 6964 1292 15894 5759 30347 1865 31042 25247 515 14392 66464 3942 2227 38356 173022',\n",
       " '49047 173020 38953 170 63548 4143 612 1422 27668 926 707 26813 48936 5929 180 10365 3815 2172 8644 5929 3604 49050 1785 173020 63548 173022',\n",
       " '42019 1630 4980 24465 35039 1238 2948 96828 5012 5701 3438 2105 35039 7 10727 9985 2770 1328 1497 782 16644 3642 32177 1438 5020 173022',\n",
       " '1499 939 13501 8913 376 86 347 72 6753 267 2423 9990 3829 19785 3301 15846 7651 2422 10094 6435 81388 169 14160 578 10003 173022',\n",
       " '90 630 839 15894 163608 1308 18313 12176 1566 20016 1635 4381 173020 14452 708 90 630 4362 4142 416 1485 418 158193 193 9645 173022',\n",
       " '2653 9380 1073 608 40 8146 1614 3989 2875 20672 1257 1630 1692 6058 956 2807 5972 16079 821 47 1485 55 8947 1443 1443 173022',\n",
       " '898 5512 473 474 149080 9137 29500 148399 841 23151 7512 1096 1172 2723 1119 3839 93 557 4879 16175 13703 391 2201 3687 880 173022',\n",
       " '126 3301 3297 17779 25658 39187 8023 1426 1091 1740 210 557 2697 1167 3338 3116 7374 3857 1782 14678 34 12372 671 7866 8830 173022',\n",
       " '10394 6678 2742 3067 1896 5638 3084 9665 2304 9657 2745 26621 9646 11618 5636 2066 3643 9675 3643 6404 939 12190 91728 46 1614 173022',\n",
       " '1003 602 634 52 992 53060 3 200 224 63348 7752 5901 160 3698 721 1984 173 2758 3810 8138 1715 70227 49201 1372 173020 173022',\n",
       " '189 8997 1072 910 18230 8566 910 3170 189 8997 6128 11445 212 56823 21174 1281 1170 5060 4576 8267 6515 2306 5060 9120 6888 173022',\n",
       " '33149 372 687 218 304 487 856 7487 108163 19872 33149 5520 1443 757 2694 515 1794 2417 6497 5678 1614 5475 10413 906 1645 173022',\n",
       " '6570 37959 6 133970 176 486 11469 304 6158 193 761 10838 7950 7052 13520 82001 26799 1805 4288 49881 133970 126371 173020 1568 173020 173022',\n",
       " '267 3033 921 3840 1445 8139 2621 34329 3241 21758 3033 921 372 326 8662 8717 21759 21760 517 12025 5651 9607 21761 21762 126 173022',\n",
       " '38939 364 2841 19961 3067 3511 1651 2465 939 38941 38942 20 46 28920 934 4965 3247 2042 6421 7910 2130 22976 2960 43360 8974 173022',\n",
       " '13566 3453 447 26790 1465 19169 2597 13566 3453 3840 10245 7795 667 1692 19169 2428 1976 1321 2005 8710 230 2232 2521 3176 1257 173022',\n",
       " '10629 1692 5847 558 5451 49 18397 2086 21005 4235 4593 8106 95685 5017 7409 595 36068 10629 416 1485 3269 12077 281 1443 3277 173022',\n",
       " '14751 4606 20122 1890 413 59 40663 1060 11894 93287 49345 5813 10057 120 6083 38011 3086 1499 1757 3942 1874 351 16186 16187 173020 173022',\n",
       " '2224 2131 285 666 3606 4731 4593 1314 97540 5885 622 3649 66 173020 10965 121045 10509 116754 1448 2173 3500 7566 4320 7131 492 173022',\n",
       " '11681 35662 79369 1711 1751 2161 9365 6327 11681 35662 2153 1704 1751 23753 495 23763 29475 9856 52108 95 3263 610 1697 2202 1874 173022',\n",
       " '8114 109014 1238 3076 12326 4717 2089 4197 3816 2428 2031 49097 91795 232 2136 53285 17076 2089 536 1504 160 27143 3269 1136 634 173022',\n",
       " '24769 20746 11392 11131 6958 9 1056 41502 49962 17478 2663 55382 1720 1839 7210 40790 2994 7185 173020 30023 2188 7853 105007 4243 42993 173022',\n",
       " '193 780 194 4587 160 1381 525 1281 1104 1814 855 1210 14393 27495 536 595 83578 1724 70020 780 15506 1117 3719 4891 4207 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '7687 281 6525 2334 3622 363 634 364 304 15031 47 15031 772 6327 3214 50723 1798 593 15014 7565 15597 10295 2534 1099 1740 173022',\n",
       " '1446 65 171 224 1769 8602 13018 2064 9719 11021 40 1406 316 2344 49 1169 2697 20636 9719 13132 2697 1434 3 125885 18204 173022',\n",
       " '4288 5347 83636 4443 29842 5449 628 413 59 7668 184 1607 1057 746 3905 4167 17088 3068 80794 115625 184 1594 406 160 406 173022',\n",
       " '40359 486 179 304 155 1899 29867 33930 17024 15109 8401 6066 5851 4920 1313 4017 3905 15497 118342 9649 173020 10719 504 7687 281 173022',\n",
       " '3120 1375 19289 1135 3146 3180 71 1896 11500 3146 9610 5656 22239 9610 5656 1649 1537 1331 273 5166 1651 283 4177 3386 1757 173022',\n",
       " '24718 5355 60473 6523 21089 15824 2301 9665 11591 3232 939 1985 111353 46 2587 533 9665 424 501 628 967 2909 5222 4495 3188 173022',\n",
       " '1661 7419 23215 59 20706 17449 17450 1314 6184 52 1995 66 536 570 30272 6561 4513 444 835 713 416 5707 416 1874 22434 173022',\n",
       " '49 801 2451 1545 173020 1519 29296 1519 29296 486 487 304 430 2451 430 349 1519 29296 486 180 304 3607 57227 1874 37204 173022',\n",
       " '25710 1965 2651 11631 9142 2162 7690 6945 4837 4017 12766 9119 26446 2183 415 2864 532 450 8057 1185 2801 9142 486 505 304 173022',\n",
       " '153 173 8017 762 291 14168 6338 368 33113 153 3810 219 1443 8167 8139 22562 33114 1445 19743 9953 250 251 14168 6338 368 173022',\n",
       " '365 7620 304 42178 517 1295 4403 10269 319 674 446 1295 301 42179 21976 3363 1469 207 521 908 725 7159 5020 86 2098 173022',\n",
       " '1273 1081 90615 8004 6257 4355 168 416 1369 55 281 414 363 369 685 1485 13750 3374 370 3383 4932 200 12442 13048 153 173022',\n",
       " '40738 5819 40739 1443 1879 3615 67937 4607 19124 304 487 2334 8754 592 171 1443 1879 2739 974 3648 193 1553 1285 489 15756 173022',\n",
       " '3241 1443 1973 65 274 811 1315 2422 3804 1285 11639 3148 59781 16983 59781 5166 19446 11240 1285 1445 2323 3148 3348 4448 59781 173022',\n",
       " '155 1337 1758 1151 850 3897 821 271 1688 1641 1294 1149 1688 1294 65 3757 2036 821 1238 571 1294 1149 1688 1358 218 173022',\n",
       " '120 46048 686 4871 1222 5714 20483 3310 32276 444 4265 3693 85484 3448 66 520 46 49 200 384 925 618 304 789 684 173022',\n",
       " '4383 487 486 304 3460 16686 6682 125 1707 154 304 173020 173020 3460 6682 125 7640 4875 304 96508 3460 20427 6682 6682 125 173022',\n",
       " '102435 1382 444 2010 87 3057 1382 444 8148 105 1408 88851 18413 1222 1432 2433 1443 102435 6337 485 65913 2229 3057 2348 10314 173022',\n",
       " '959 12726 1629 755 13404 4237 34046 21581 590 3261 93 105 6018 13404 285 789 202 6018 14897 755 4001 2949 1925 8684 24893 173022',\n",
       " '141863 12242 4129 4133 4395 13818 105812 3487 14606 1856 413 566 10476 160 10059 2199 173020 1563 17543 173020 7857 22222 1485 3146 20946 173022',\n",
       " '156046 26507 56101 12741 11942 573 634 4378 17009 402 40794 72237 12893 60143 1997 4705 2793 16 16620 5298 16781 17940 6084 18206 9594 173022',\n",
       " '2619 2221 86 6567 6567 20814 349 255 3203 9084 428 86 430 429 6791 762 1336 20814 173020 321 61511 1398 451 1720 762 173022',\n",
       " '14531 772 6207 4799 536 8511 9700 5876 4784 346 1792 3269 1706 1060 5715 1160 2758 271 25130 10169 73017 158648 1284 8104 790 173022',\n",
       " '3813 10710 18477 21585 5032 12988 451 676 4658 34586 7551 8748 1726 45 451 676 1445 4658 219 153 1445 1285 1615 86140 350 173022',\n",
       " '3135 9307 5744 2500 979 6673 4111 6722 2764 174 10034 8584 3425 8434 9307 55818 29329 979 6569 10151 6511 3919 10154 416 51528 173022',\n",
       " '173020 5526 86 36756 46 9355 9355 571 5023 11491 3684 46 862 2160 7032 1844 9355 9355 2160 3466 3323 5606 671 759 120 173022',\n",
       " '7870 867 10745 6531 384 173020 173020 3328 701 508 10334 153 10335 10336 903 10337 867 10745 1061 7870 10745 2653 7993 51533 19334 173022',\n",
       " '153 55 173020 22687 34491 20412 7813 7260 10750 6648 486 304 8405 591 439 86 20918 50777 58510 31994 11775 65 759 702 37238 173022',\n",
       " '346 1792 56039 1960 4916 2463 351 671 1443 22047 349 32150 583 86 2925 851 557 258 3593 7069 3033 2271 173020 346 321 173022',\n",
       " '1984 2620 23225 6373 4207 17310 194 3852 10411 5558 1176 13905 3502 3034 1491 3169 7947 937 3185 8359 3429 424 2788 321 1615 173022',\n",
       " '37143 34 10594 13486 28547 3798 37142 3826 5268 926 20185 1543 1758 200 13486 2584 12009 37144 31902 37143 3939 4133 173020 49210 98229 173022',\n",
       " '81093 7147 81093 17612 1245 8943 1637 101 8449 3852 21674 1982 6822 165013 1751 18873 1091 2191 2771 451 5700 8245 2075 3642 2772 173022',\n",
       " '767 4362 7851 2297 3712 140 2815 5885 57 2551 2136 9123 18289 34957 1463 7851 18250 287 7029 17008 6558 18765 173020 88442 349 173022',\n",
       " '138682 138683 18883 33706 486 505 99109 138682 138683 4341 6006 61446 173020 8471 8898 41164 155868 58901 205 5065 16 416 583 6571 432 173022',\n",
       " '5210 6076 5869 2803 6868 1073 8510 41244 4160 2654 16661 2807 1567 25864 131355 50601 1523 5123 2803 30364 10602 2355 1630 1742 15197 173022',\n",
       " '91790 48450 1384 3213 382 3696 864 91790 1313 173020 15780 7609 1016 657 1791 91790 48450 1408 713 382 864 1463 3674 49 2427 173022',\n",
       " '2055 457 4073 287 451 775 3879 92031 93 4602 333 11065 1092 17571 14005 821 5063 6825 513 4036 3971 1174 14005 89 1535 173022',\n",
       " '1664 14705 502 396 12766 7651 20162 49087 15497 22786 7813 3905 94 27991 1136 13882 80720 15497 102576 2807 22786 1093 22786 1857 8367 173022',\n",
       " '384 178 7640 180 618 3328 2320 2726 611 179 20333 514 319 2650 6790 5213 18912 1737 25325 7258 4183 2587 488 490 29260 173022',\n",
       " '7093 1443 1672 2096 1491 3671 17981 110827 7093 1443 12141 1491 10857 6310 51573 1709 2943 2140 1984 4372 2521 8736 14742 206 5674 173022',\n",
       " '224 3 72 2025 160 17570 283 18104 537 634 683 39421 9936 11614 12182 634 155 39421 9936 4262 3525 683 676 505 39421 173022',\n",
       " '7716 1472 5000 6059 3712 3915 2427 2428 30439 3067 3507 1076 200 1523 2844 1316 2367 4995 9707 2866 1965 8081 23181 33533 1451 173022',\n",
       " '4166 20287 1200 4166 3139 676 2369 312 9757 3 4274 28627 6749 2650 5174 622 4166 5504 1375 23 2711 487 179 4747 2131 173022',\n",
       " '8780 4288 1652 6777 3968 3969 140 2574 932 378 4595 10093 1171 16319 926 7097 55 2407 2408 5714 1883 32514 46808 470 5330 173022',\n",
       " '430 1273 2010 560 2194 120 10108 4750 184 285 195 161 403 22 208 8152 6790 632 9111 814 71357 3269 759 1607 2306 173022',\n",
       " '173020 17517 6061 2041 23600 27450 2640 1113 73 23601 184 2933 486 2793 304 1438 23600 416 1288 173020 1163 416 1369 3269 321 173022',\n",
       " '17154 190 1278 237 3777 3665 405 3708 4909 2517 4441 82952 44368 405 65325 7687 4909 33720 120 1705 4013 4441 9996 4358 289 173022',\n",
       " '3892 1671 10779 120 41089 970 3628 2012 2697 4654 3086 184 6813 2025 160 10779 120 41089 4653 3852 486 2793 416 1438 96058 173022',\n",
       " '25043 1491 4306 1499 939 13501 8913 347 2351 335 27147 14328 25043 1997 21348 52841 7122 71758 9676 335 1327 801 2794 1700 142132 173022',\n",
       " '2619 2221 153 1874 2619 2221 429 430 4634 1784 509 4855 4905 153 504 2711 155 5700 3841 7079 6871 87621 11261 6613 26397 173022',\n",
       " '1875 40738 4207 65 274 7127 2301 12095 4895 2422 3804 82532 4448 8000 3610 2229 9632 9251 2014 3348 2013 211 5410 2637 7374 173022',\n",
       " '35222 1492 369 1827 15688 211 12775 35224 25371 1904 5445 4892 412 2459 4168 5001 194 193 6867 20186 1907 13611 35225 33641 6061 173022',\n",
       " '2334 710 629 7640 3622 3940 2333 3057 1970 36221 3632 7377 173020 3607 449 178 123409 52 1238 789 157 2311 489 112 319 173022',\n",
       " '7166 2832 20431 3425 5987 9289 4364 7299 2758 2650 15182 7166 571 20431 3425 2671 173020 153 55 5255 634 39529 4505 55 281 173022',\n",
       " '1875 40738 4207 65 274 3896 1315 2422 3804 1285 11639 3148 59781 16983 59781 5166 19446 11240 1285 1445 2323 3148 3348 4448 59781 173022',\n",
       " '8183 1285 4658 48460 3922 628 753 967 2909 3504 581 536 413 867 7039 224 30050 267 3241 3564 63 1505 173 2191 2587 173022',\n",
       " '1416 3237 18811 26127 7213 296 1340 93 24 2287 194 1856 193 790 26127 6290 152215 9501 40564 672 5448 3862 1281 1295 173020 173022',\n",
       " '591 19221 1630 451 32996 3411 7419 1981 33076 1982 1796 3853 5033 1516 1999 2000 2406 1967 6805 3452 1217 1118 2117 321 1615 173022',\n",
       " '31956 4664 4000 7313 160 7857 153 55 29265 2157 1918 3587 11847 136219 3504 503 1408 2224 794 4418 10909 21571 21572 12051 7360 173022',\n",
       " '173020 6947 74404 2492 15154 14868 20746 24769 11131 2224 1650 147266 4761 5833 9492 5839 7879 3034 368 255 1857 416 1438 4619 2433 173022',\n",
       " '184 12706 11494 24974 1492 37247 1657 6637 4614 976 3306 3141 4372 4374 7616 19459 274 14457 13516 927 307 19758 32164 334 173020 173022',\n",
       " '20251 1298 9181 12713 34349 302 7651 1486 760 2135 7651 1486 11681 10273 2579 675 6746 2579 1485 1280 9111 34247 1226 557 2492 173022',\n",
       " '8899 2280 8023 23013 173020 27792 71597 2008 3950 4651 378 1571 1553 5405 4013 525 5330 32665 250 71597 4939 83012 1641 31099 36091 173022',\n",
       " '996 2130 13869 13870 20 15552 683 330 160 2465 2538 1138 6456 5586 347 15038 7623 5194 267 1616 3306 52253 683 1921 514 173022',\n",
       " '421 610 1865 6408 13882 571 1860 1768 610 446 120 3338 7840 1094 1427 4304 410 87 1463 3338 444 403 220 319 571 173022',\n",
       " '958 2409 26815 1294 5688 782 2031 90804 956 6838 489 170 6074 1997 5493 5037 1442 789 7620 20584 18506 958 15581 2413 26815 173022',\n",
       " '54424 12968 439 7867 95959 10 126 76 6841 654 664 25744 20463 13397 11576 39664 9037 173020 12968 3451 9595 2723 30622 726 1496 173022',\n",
       " '3370 96123 16753 6613 20211 1239 19124 591 3370 2567 3262 3850 22730 76693 1239 19124 591 3764 3697 56647 67186 3850 5063 11847 4340 173022',\n",
       " '421 2221 1800 4580 486 304 4796 49 591 4527 29473 9177 3261 43577 21104 832 2218 11822 5718 1709 600 9301 7259 9342 113 173022',\n",
       " '3504 487 304 81149 4367 21208 454 45734 3942 2030 39 46777 1007 10084 14122 321 350 634 8247 2021 3418 3418 193 5020 418 173022',\n",
       " '23146 12077 674 2653 29957 1827 3410 7419 15837 1630 17439 17440 236 14869 17438 8091 3955 2758 1874 30756 1443 351 16186 16187 173020 173022',\n",
       " '13404 2277 755 756 6269 810 17901 2856 818 819 15339 1492 3905 4167 3712 746 11946 999 1227 9116 18285 7032 819 2937 2937 173022',\n",
       " '779 4004 504 153 504 153 153 41653 5475 374 8192 37673 8272 6242 20083 11701 173020 224 4498 7207 14444 2534 160 24196 8272 173022',\n",
       " '7816 525 2366 7729 9521 3343 3189 1572 93 1904 1180 3673 1216 7117 5765 10608 9791 1180 20842 7374 21369 19439 7374 32410 7374 173022',\n",
       " '31768 9555 3609 750 5055 17511 6313 1693 5732 3411 7419 17449 17450 46 3852 9194 5800 3791 3334 13490 14953 76 44 1551 7969 173022',\n",
       " '14392 780 20733 2665 160 1095 6327 2025 1921 8890 4324 160 4891 723 120 8901 160 1298 281 5662 347 289 1369 7097 281 173022',\n",
       " '2974 10210 13929 1136 2384 1630 13921 1542 56700 7749 31727 37928 23493 86912 1273 7663 110800 2384 1630 13921 1542 56700 7749 31727 37928 173022',\n",
       " '1485 281 283 3237 11667 521 11683 13931 5027 1957 5620 4340 31177 1432 33607 4340 31177 3772 490 1862 95001 31177 3772 95002 95001 173022',\n",
       " '104921 4988 1918 6515 10613 2552 15066 7558 2955 15066 7722 1954 2955 4769 18526 1692 4460 2240 1094 9046 1549 52 202 557 1577 173022',\n",
       " '76205 88119 28875 2723 204 1315 7542 7659 557 5574 16156 7742 2931 516 184 106 49 1169 2931 6884 4576 3502 4951 39388 6570 173022',\n",
       " '1187 6327 2170 2136 6749 6066 2211 58998 232 2136 58998 6127 27858 11896 173020 3942 18081 1577 396 1421 6066 321 917 153 1708 173022',\n",
       " '40738 5819 40739 49325 1652 2784 49327 1273 12425 304 179 974 8754 618 171 49325 1652 2739 790 1895 193 1553 1285 170 2079 173022',\n",
       " '683 1921 514 1576 1136 1355 14 73812 3150 339 1449 14580 428 4784 15151 3150 917 223 2165 444 1553 1443 8899 1784 430 173022',\n",
       " '2130 7657 1711 8221 70754 70755 48211 5046 4091 147864 14432 13522 2094 5938 7657 7304 9645 8241 1984 179 316 5342 2130 28096 9739 173022',\n",
       " '758 63798 87404 34306 7689 1709 578 3490 713 1108 319 418 758 87404 3696 87 30525 22559 444 63798 7549 1434 3 439 30525 173022',\n",
       " '16373 4642 2685 66453 2196 4195 578 1524 462 2856 173020 1757 113 1373 3342 160 633 5936 17947 3836 11822 628 683 321 1615 173022',\n",
       " '2418 3609 1913 6275 10281 1150 71 676 202 14778 18036 757 10321 749 5897 307 10278 2398 3425 3426 814 2945 160 10281 5405 173022',\n",
       " '32853 302 5159 571 1913 6234 416 1369 55 281 414 363 671 591 1485 13750 3374 370 3383 4932 200 12442 13048 153 1968 173022',\n",
       " '120 15207 4733 3086 7017 58968 8858 51232 440 6721 8864 3088 2130 13509 13510 17001 1885 2146 34 6706 45 677 26785 129788 120 173022',\n",
       " '7651 3776 2156 4448 3852 320 3162 7491 3776 1965 173 2191 8840 2925 1913 3200 5333 5565 1549 1574 202 3504 917 1485 55 173022',\n",
       " '27260 27261 18189 1240 20799 5944 4448 1913 2075 1827 2080 634 1055 1408 2407 2408 4652 2223 18890 30739 618 155 1800 321 1615 173022',\n",
       " '40738 5819 40739 103734 103735 2784 103736 4607 19227 304 170 385 8754 618 171 103734 103735 2739 693 1055 193 1553 1285 753 5246 173022',\n",
       " '40738 5819 40739 36764 3241 2784 22498 1442 12425 304 3669 52 8754 683 171 36764 3241 2739 1044 1970 193 1553 1285 727 11334 173022',\n",
       " '40738 5819 40739 30666 3338 2784 21815 1504 21810 2844 789 11334 8754 753 171 30666 3338 2739 503 489 193 1553 1285 505 3649 173022',\n",
       " '740 741 2635 2636 95 757 1796 740 741 604 3559 95 757 1796 2635 2628 604 194 1368 2130 51 202 772 4013 6327 173022',\n",
       " '790 591 52033 1117 23507 23032 14979 27488 24864 5213 47786 33370 71568 173020 173020 42223 1904 8661 4891 1650 1542 8899 160 8936 33926 173022',\n",
       " '34299 6327 1574 2475 13471 5941 3213 469 21653 3583 102954 22434 173020 384 62928 3632 990 508 10334 153 10335 10336 903 10337 7647 173022',\n",
       " '209 1614 3559 4939 113 720 56369 62716 232 3253 8505 1649 2872 403 851 715 2556 4224 1639 19791 3559 4939 5881 451 9974 173022',\n",
       " '55171 47184 11201 65 11177 13495 25572 3047 1492 1090 144 1679 1785 21568 1338 20132 47185 3 999 2316 1811 1090 25572 13145 9553 173022',\n",
       " '40738 5819 40739 25274 3385 3615 25274 1273 7260 304 487 15756 8754 618 171 25274 3385 2739 2841 998 193 1553 1285 489 155 173022',\n",
       " '2608 2609 2994 193 610 2587 2661 160 4837 2229 1237 409 697 580 173020 1870 610 424 1844 11667 2608 3270 774 9864 11491 173022',\n",
       " '58 14513 4244 25570 4331 1189 17652 9044 25883 11839 781 947 1708 4559 3323 13697 37391 517 17652 9044 12246 58 9446 30911 3585 173022',\n",
       " '54424 12968 439 7867 95959 10 126 76 6841 654 664 25744 20463 13397 11576 39664 9037 173020 12968 3451 9595 2723 30622 726 1496 173022',\n",
       " '1703 4121 671 3287 14139 2088 1784 27823 106928 89444 22974 7500 811 6805 969 1594 6364 14693 2433 1267 557 258 3502 3490 5759 173022',\n",
       " '1856 413 316 2427 5334 415 267 3269 7223 3237 42903 2437 106211 2366 1034 266 2797 34 16634 2797 2795 363 58004 1973 193 173022',\n",
       " '1968 3610 3241 67049 54202 80862 57545 72454 17551 31919 1984 17827 31920 80862 490 3338 31920 80862 490 4341 6476 31920 21178 17827 31920 173022',\n",
       " '2199 173020 2132 6569 4845 1508 13477 618 506 1762 24911 13477 9142 173020 34 11531 13477 2313 21799 1724 76032 2685 44828 6569 1762 173022',\n",
       " '18344 5987 3165 200 105 486 52 304 2335 39428 5360 33122 9636 2776 1711 595 683 495 11760 153 1688 69996 856 1690 3354 173022']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for j in range(50000, 50500):\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer_logits = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([str(i) for i in answer_logits if i != 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer_logits = __loadStuff(\"answer_logits.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "for i in range(len(answer_logits)):\n",
    "    answers.append(answer_logits[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = np.asarray(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212289, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = np.argmax(answers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 14637, 1: 197652})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(list(yo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 9], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.argpartition(answers[:10][:, 1], -2)[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_summaries = [[]]\n",
    "abs_output = pd.DataFrame(columns = ['document', 'summary', 'predicted_summary'])\n",
    "\n",
    "abs_output[\"document\"] = desc\n",
    "abs_output[\"summary\"] = heads\n",
    "\n",
    "# for i in range(len(test_output[\"document\"])):\n",
    "#     temp = []\n",
    "#     for j in range(len(test_output[\"document\"][i])):\n",
    "#         if int(test_output[\"targets\"][i][j]) == 1:\n",
    "#             temp.append(test_docs[i][j])\n",
    "#     test_output[\"predicted_summary\"][i] = temp\n",
    "\n",
    "# count = 0\n",
    "# counts = 0\n",
    "# for i in range(len(test_output[\"document\"])):\n",
    "#     temp = []\n",
    "#     for j in range(len(test_output[\"document\"][i])):\n",
    "#         if int(yo[count-1]) == 1 and answers[count-1][1] > 0.99:\n",
    "#             temp.append(test_output[\"document\"][i][j])\n",
    "#         count += 1\n",
    "#     pers_output[\"predicted_summary\"][i] = temp\n",
    "\n",
    "c = 0\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    temp = []\n",
    "    index = list(range(c, c+len(documents[i])))\n",
    "    try:\n",
    "        index = np.sort(np.argpartition(answers[index][:, 1], -1)[-1:])\n",
    "        for j in range(len(documents[i])):\n",
    "            if j in index:\n",
    "                temp.append(documents[i][j])\n",
    "        abs_output[\"predicted_summary\"][i] = temp\n",
    "    except:\n",
    "        abs_output[\"predicted_summary\"][i] = np.nan\n",
    "    \n",
    "    c += len(documents[i])\n",
    "    \n",
    "    \n",
    "for i in range(len(abs_output[\"predicted_summary\"])):\n",
    "    try:\n",
    "        abs_output[\"predicted_summary\"][i] = \". \".join(abs_output[\"predicted_summary\"][i])\n",
    "    except:\n",
    "        abs_output[\"predicted_summary\"][i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>predicted_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>48807</td>\n",
       "      <td>48776</td>\n",
       "      <td>48178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>stock market xml and json data api provided by...</td>\n",
       "      <td>my tweets</td>\n",
       "      <td>stock market xml and json data api provided by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>79</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 document    summary  \\\n",
       "count                                               50000      50000   \n",
       "unique                                              48807      48776   \n",
       "top     stock market xml and json data api provided by...  my tweets   \n",
       "freq                                                   79         17   \n",
       "\n",
       "                                        predicted_summary  \n",
       "count                                               50000  \n",
       "unique                                              48178  \n",
       "top     stock market xml and json data api provided by...  \n",
       "freq                                                   79  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_output = abs_output.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_output = abs_output.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps, refs = map(list, zip(*[[abs_output[\"predicted_summary\"][i], abs_output[\"summary\"][i]] for i in range(len(abs_output))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('many of the 5.5 million voters had said on sunday that they did not believe catalonia would become independent and had used their ballot as a way to press the catalan and spanish authorities to discuss those issues.\\xa0 \\n    \\n- reuters',\n",
       " 'exit poll shows separatists winning majority in catalan assembly')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps[122], refs[122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps, refs, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.08966479912909088,\n",
       "  'p': 0.05692064123090016,\n",
       "  'r': 0.49931013603166086},\n",
       " 'rouge-2': {'f': 0.03386589739702842,\n",
       "  'p': 0.02172550094717754,\n",
       "  'r': 0.22539446117059553},\n",
       " 'rouge-l': {'f': 0.041923081397736595,\n",
       "  'p': 0.040769395968043555,\n",
       "  'r': 0.4387247398939762}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
