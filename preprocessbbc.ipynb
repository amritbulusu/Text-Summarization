{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import stopwords\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['directory','headline','text'])\n",
    "c = 0\n",
    "cwd = 'C:\\\\Users\\\\amrit\\\\OneDrive\\\\Desktop\\\\Thesis\\\\Data\\\\Dataset\\\\bbc'\n",
    "for filename in os.listdir(cwd):\n",
    "    if '.' not in filename:\n",
    "        for fname in os.listdir(cwd+'\\\\'+filename):\n",
    "            path = cwd+'\\\\'+filename+'\\\\'+fname\n",
    "            with open(path, encoding='ISO-8859-1') as ftext:\n",
    "                headline = ftext.readline()\n",
    "                text = ftext.read()\n",
    "                headline = headline.replace('\\n',' ')\n",
    "                text = text.replace('\\n',' ')\n",
    "                record = [filename,headline,text]\n",
    "                df.loc[c] = record\n",
    "                c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>Japan narrowly escapes recession</td>\n",
       "      <td>Japan's economy teetered on the brink of a te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>Jobs growth still slow in the US</td>\n",
       "      <td>The US created fewer jobs than expected in Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>India calls for fair trade rules</td>\n",
       "      <td>India, which attends the G7 meeting of seven ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>Ethiopia's crop production up 24%</td>\n",
       "      <td>Ethiopia produced 14.27 million tonnes of cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>Court rejects $280bn tobacco case</td>\n",
       "      <td>A US government claim accusing the country's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>business</td>\n",
       "      <td>Ask Jeeves tips online ad revival</td>\n",
       "      <td>Ask Jeeves has become the third leading onlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>business</td>\n",
       "      <td>Indonesians face fuel price rise</td>\n",
       "      <td>Indonesia's government has confirmed it is co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>business</td>\n",
       "      <td>Peugeot deal boosts Mitsubishi</td>\n",
       "      <td>Struggling Japanese car maker Mitsubishi Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business</td>\n",
       "      <td>Telegraph newspapers axe 90 jobs</td>\n",
       "      <td>The Daily and Sunday Telegraph newspapers are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>business</td>\n",
       "      <td>Air passengers win new EU rights</td>\n",
       "      <td>Air passengers who are unable to board their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>business</td>\n",
       "      <td>China keeps tight rein on credit</td>\n",
       "      <td>China's efforts to stop the economy from over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>business</td>\n",
       "      <td>Parmalat boasts doubled profits</td>\n",
       "      <td>Parmalat, the Italian food group at the centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>business</td>\n",
       "      <td>India's rupee hits five-year high</td>\n",
       "      <td>India's rupee has hit a five-year high after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>business</td>\n",
       "      <td>India widens access to telecoms</td>\n",
       "      <td>India has raised the limit for foreign direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>business</td>\n",
       "      <td>Call centre users 'lose patience'</td>\n",
       "      <td>Customers trying to get through to call centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>business</td>\n",
       "      <td>Rank 'set to sell off film unit'</td>\n",
       "      <td>Leisure group Rank could unveil plans to deme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>business</td>\n",
       "      <td>Sluggish economy hits German jobs</td>\n",
       "      <td>The number of people out of work in Europe's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>business</td>\n",
       "      <td>Mixed signals from French economy</td>\n",
       "      <td>The French economy picked up speed at the end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>business</td>\n",
       "      <td>US trade gap hits record in 2004</td>\n",
       "      <td>The gap between US exports and imports hit an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos loses US bankruptcy battle</td>\n",
       "      <td>A judge has dismissed an attempt by Russian o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>business</td>\n",
       "      <td>Safety alert as GM recalls cars</td>\n",
       "      <td>The world's biggest carmaker General Motors (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>business</td>\n",
       "      <td>Steel firm 'to cut' 45,000 jobs</td>\n",
       "      <td>Mittal Steel, one of the world's largest stee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>business</td>\n",
       "      <td>Strong demand triggers oil rally</td>\n",
       "      <td>Crude oil prices surged back above the $47 a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>business</td>\n",
       "      <td>UK firm faces Venezuelan land row</td>\n",
       "      <td>Venezuelan authorities have said they will se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>business</td>\n",
       "      <td>Soaring oil 'hits world economy'</td>\n",
       "      <td>The soaring cost of oil has hit global econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>tech</td>\n",
       "      <td>Text message record smashed again</td>\n",
       "      <td>UK mobile owners continue to break records wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>tech</td>\n",
       "      <td>Software watching while you work</td>\n",
       "      <td>Software that can not only monitor every keys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>tech</td>\n",
       "      <td>Commodore finds new lease of life</td>\n",
       "      <td>The once-famous Commodore computer brand coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>tech</td>\n",
       "      <td>Cabs collect mountain of mobiles</td>\n",
       "      <td>Gadgets are cheaper, smaller and more common ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>tech</td>\n",
       "      <td>T-Mobile bets on 'pocket office'</td>\n",
       "      <td>T-Mobile has launched its latest \"pocket offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>tech</td>\n",
       "      <td>California sets fines for spyware</td>\n",
       "      <td>The makers of computer programs that secretly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>tech</td>\n",
       "      <td>Mobile TV tipped as one to watch</td>\n",
       "      <td>Scandinavians and Koreans, two of the most ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>tech</td>\n",
       "      <td>Apple laptop is 'greatest gadget'</td>\n",
       "      <td>The Apple Powerbook 100 has been chosen as th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>tech</td>\n",
       "      <td>Sun offers processing by the hour</td>\n",
       "      <td>Sun Microsystems has launched a pay-as-you-go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>tech</td>\n",
       "      <td>Kenyan school turns to handhelds</td>\n",
       "      <td>At the Mbita Point primary school in western ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>tech</td>\n",
       "      <td>Tough rules for ringtone sellers</td>\n",
       "      <td>Firms that flout rules on how ringtones and o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>tech</td>\n",
       "      <td>Mobile music challenges 'iPod age'</td>\n",
       "      <td>Nokia and Microsoft have agreed a deal to wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>tech</td>\n",
       "      <td>China 'ripe' for media explosion</td>\n",
       "      <td>Asia is set to drive global media growth to 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>tech</td>\n",
       "      <td>Beckham virus spotted on the net</td>\n",
       "      <td>Virus writers are trading on interest in Davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>tech</td>\n",
       "      <td>Video phones act as dating tools</td>\n",
       "      <td>Technologies, from e-mail, to net chatrooms, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>tech</td>\n",
       "      <td>Progress on new internet domains</td>\n",
       "      <td>By early 2005 the net could have two new doma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>tech</td>\n",
       "      <td>Camera phones are 'must-haves'</td>\n",
       "      <td>Four times more mobiles with cameras in them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>tech</td>\n",
       "      <td>Mobile multimedia slow to catch on</td>\n",
       "      <td>There is no doubt that mobile phones sporting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>tech</td>\n",
       "      <td>Anti-spam laws bite spammer hard</td>\n",
       "      <td>The net's self-declared spam king is seeking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>tech</td>\n",
       "      <td>Peer-to-peer nets 'here to stay'</td>\n",
       "      <td>Peer-to-peer (P2P) networks are here to stay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>tech</td>\n",
       "      <td>Broadband fuels online expression</td>\n",
       "      <td>Fast web access is encouraging more people to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>tech</td>\n",
       "      <td>Savvy searchers fail to spot ads</td>\n",
       "      <td>Internet search engine users are an odd mix o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>tech</td>\n",
       "      <td>TV's future down the phone line</td>\n",
       "      <td>Internet TV has been talked about since the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>Cebit fever takes over Hanover</td>\n",
       "      <td>Thousands of products and tens of thousands o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>New consoles promise big problems</td>\n",
       "      <td>Making games for future consoles will require...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>tech</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>tech</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2227 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     directory                             headline  \\\n",
       "0     business   Ad sales boost Time Warner profit    \n",
       "1     business    Dollar gains on Greenspan speech    \n",
       "2     business   Yukos unit buyer faces loan claim    \n",
       "3     business   High fuel prices hit BA's profits    \n",
       "4     business   Pernod takeover talk lifts Domecq    \n",
       "5     business    Japan narrowly escapes recession    \n",
       "6     business    Jobs growth still slow in the US    \n",
       "7     business    India calls for fair trade rules    \n",
       "8     business   Ethiopia's crop production up 24%    \n",
       "9     business   Court rejects $280bn tobacco case    \n",
       "10    business   Ask Jeeves tips online ad revival    \n",
       "11    business    Indonesians face fuel price rise    \n",
       "12    business      Peugeot deal boosts Mitsubishi    \n",
       "13    business    Telegraph newspapers axe 90 jobs    \n",
       "14    business    Air passengers win new EU rights    \n",
       "15    business    China keeps tight rein on credit    \n",
       "16    business     Parmalat boasts doubled profits    \n",
       "17    business   India's rupee hits five-year high    \n",
       "18    business     India widens access to telecoms    \n",
       "19    business   Call centre users 'lose patience'    \n",
       "20    business    Rank 'set to sell off film unit'    \n",
       "21    business   Sluggish economy hits German jobs    \n",
       "22    business   Mixed signals from French economy    \n",
       "23    business    US trade gap hits record in 2004    \n",
       "24    business    Yukos loses US bankruptcy battle    \n",
       "25    business     Safety alert as GM recalls cars    \n",
       "26    business     Steel firm 'to cut' 45,000 jobs    \n",
       "27    business    Strong demand triggers oil rally    \n",
       "28    business   UK firm faces Venezuelan land row    \n",
       "29    business    Soaring oil 'hits world economy'    \n",
       "...        ...                                  ...   \n",
       "2197      tech   Text message record smashed again    \n",
       "2198      tech    Software watching while you work    \n",
       "2199      tech   Commodore finds new lease of life    \n",
       "2200      tech    Cabs collect mountain of mobiles    \n",
       "2201      tech    T-Mobile bets on 'pocket office'    \n",
       "2202      tech   California sets fines for spyware    \n",
       "2203      tech    Mobile TV tipped as one to watch    \n",
       "2204      tech   Apple laptop is 'greatest gadget'    \n",
       "2205      tech   Sun offers processing by the hour    \n",
       "2206      tech    Kenyan school turns to handhelds    \n",
       "2207      tech    Tough rules for ringtone sellers    \n",
       "2208      tech  Mobile music challenges 'iPod age'    \n",
       "2209      tech    China 'ripe' for media explosion    \n",
       "2210      tech    Beckham virus spotted on the net    \n",
       "2211      tech    Video phones act as dating tools    \n",
       "2212      tech    Progress on new internet domains    \n",
       "2213      tech      Camera phones are 'must-haves'    \n",
       "2214      tech  Mobile multimedia slow to catch on    \n",
       "2215      tech    Anti-spam laws bite spammer hard    \n",
       "2216      tech    Peer-to-peer nets 'here to stay'    \n",
       "2217      tech   Broadband fuels online expression    \n",
       "2218      tech    Savvy searchers fail to spot ads    \n",
       "2219      tech     TV's future down the phone line    \n",
       "2220      tech      Cebit fever takes over Hanover    \n",
       "2221      tech   New consoles promise big problems    \n",
       "2222      tech    BT program to beat dialler scams    \n",
       "2223      tech     Spam e-mails tempt net shoppers    \n",
       "2224      tech             Be careful how you code    \n",
       "2225      tech     US cyber security chief resigns    \n",
       "2226      tech    Losing yourself in online gaming    \n",
       "\n",
       "                                                   text  \n",
       "0      Quarterly profits at US media giant TimeWarne...  \n",
       "1      The dollar has hit its highest level against ...  \n",
       "2      The owners of embattled Russian oil giant Yuk...  \n",
       "3      British Airways has blamed high fuel prices f...  \n",
       "4      Shares in UK drinks and food firm Allied Dome...  \n",
       "5      Japan's economy teetered on the brink of a te...  \n",
       "6      The US created fewer jobs than expected in Ja...  \n",
       "7      India, which attends the G7 meeting of seven ...  \n",
       "8      Ethiopia produced 14.27 million tonnes of cro...  \n",
       "9      A US government claim accusing the country's ...  \n",
       "10     Ask Jeeves has become the third leading onlin...  \n",
       "11     Indonesia's government has confirmed it is co...  \n",
       "12     Struggling Japanese car maker Mitsubishi Moto...  \n",
       "13     The Daily and Sunday Telegraph newspapers are...  \n",
       "14     Air passengers who are unable to board their ...  \n",
       "15     China's efforts to stop the economy from over...  \n",
       "16     Parmalat, the Italian food group at the centr...  \n",
       "17     India's rupee has hit a five-year high after ...  \n",
       "18     India has raised the limit for foreign direct...  \n",
       "19     Customers trying to get through to call centr...  \n",
       "20     Leisure group Rank could unveil plans to deme...  \n",
       "21     The number of people out of work in Europe's ...  \n",
       "22     The French economy picked up speed at the end...  \n",
       "23     The gap between US exports and imports hit an...  \n",
       "24     A judge has dismissed an attempt by Russian o...  \n",
       "25     The world's biggest carmaker General Motors (...  \n",
       "26     Mittal Steel, one of the world's largest stee...  \n",
       "27     Crude oil prices surged back above the $47 a ...  \n",
       "28     Venezuelan authorities have said they will se...  \n",
       "29     The soaring cost of oil has hit global econom...  \n",
       "...                                                 ...  \n",
       "2197   UK mobile owners continue to break records wi...  \n",
       "2198   Software that can not only monitor every keys...  \n",
       "2199   The once-famous Commodore computer brand coul...  \n",
       "2200   Gadgets are cheaper, smaller and more common ...  \n",
       "2201   T-Mobile has launched its latest \"pocket offi...  \n",
       "2202   The makers of computer programs that secretly...  \n",
       "2203   Scandinavians and Koreans, two of the most ad...  \n",
       "2204   The Apple Powerbook 100 has been chosen as th...  \n",
       "2205   Sun Microsystems has launched a pay-as-you-go...  \n",
       "2206   At the Mbita Point primary school in western ...  \n",
       "2207   Firms that flout rules on how ringtones and o...  \n",
       "2208   Nokia and Microsoft have agreed a deal to wor...  \n",
       "2209   Asia is set to drive global media growth to 2...  \n",
       "2210   Virus writers are trading on interest in Davi...  \n",
       "2211   Technologies, from e-mail, to net chatrooms, ...  \n",
       "2212   By early 2005 the net could have two new doma...  \n",
       "2213   Four times more mobiles with cameras in them ...  \n",
       "2214   There is no doubt that mobile phones sporting...  \n",
       "2215   The net's self-declared spam king is seeking ...  \n",
       "2216   Peer-to-peer (P2P) networks are here to stay,...  \n",
       "2217   Fast web access is encouraging more people to...  \n",
       "2218   Internet search engine users are an odd mix o...  \n",
       "2219   Internet TV has been talked about since the s...  \n",
       "2220   Thousands of products and tens of thousands o...  \n",
       "2221   Making games for future consoles will require...  \n",
       "2222   BT is introducing two initiatives to help bea...  \n",
       "2223   Computer users across the world continue to i...  \n",
       "2224   A new European directive could put software w...  \n",
       "2225   The man making sure US computer networks are ...  \n",
       "2226   Online role playing games are time-consuming,...  \n",
       "\n",
       "[2227 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values_frame = pd.DataFrame(columns=['Total','Match'])\n",
    "for i in range(0,len(df)):\n",
    "    counter = 0\n",
    "    data = df.iloc[i,1:].values\n",
    "    head_split = data[0].split()\n",
    "    total = len(head_split)\n",
    "    for j in range(0,total):\n",
    "        if len(re.findall(head_split[j],data[1],re.IGNORECASE))>0:\n",
    "            counter+=1\n",
    "    values_frame.loc[i] = [total,counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'business'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2394c4201b57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'business'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'business'"
     ]
    }
   ],
   "source": [
    "os.listdir('business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values_frame['percentage'] = (values_frame['Match']/values_frame['Total'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of words in headline present in the text before stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.90128410914933"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(values_frame['percentage'])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"headline\"] = stemming(df[\"headline\"])\n",
    "df[\"text\"] = stemming(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values_frame = pd.DataFrame(columns=['Total','Match'])\n",
    "for i in range(0,len(df)):\n",
    "    counter = 0\n",
    "    data = df.iloc[i,1:].values\n",
    "    head_split = data[0].split()\n",
    "    total = len(head_split)\n",
    "    for j in range(0,total):\n",
    "        if len(re.findall(head_split[j],data[1],re.IGNORECASE))>0:\n",
    "            counter+=1\n",
    "    values_frame.loc[i] = [total,counter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of words in headline present in the text after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.2477795612628"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_frame['percentage'] = (values_frame['Match']/values_frame['Total'])*100\n",
    "sum(values_frame['percentage'])/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(documents, stopwords_removal=False, stemming=False):\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    for i in range(len(documents)):\n",
    "        documents[i] = documents[i].lower()\n",
    "        documents[i] = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', documents[i])\n",
    "        documents[i] = re.findall(r\"[\\w']+\", documents[i])\n",
    "        new_text = []\n",
    "        for word in documents[i]:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        documents[i] = \" \".join(new_text)\n",
    "        tokens = documents[i].split()\n",
    "        words = []\n",
    "        if stopwords_removal:\n",
    "            for token in tokens:\n",
    "                if token not in cachedStopWords:\n",
    "                    words.append(token)\n",
    "                documents[i] = \" \".join(words)        \n",
    "    if stemming:\n",
    "        documents = [\" \".join([stem(word) for word in sentence.split(\" \")]) for sentence in documents]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ask jeeves become third leading online search firm week thank revival internet advertising improving fortunes firm's revenue nearly tripled fourth quarter 2004 exceeding 86m â 46m ask jeeves among best known names web relatively modest player 17m profit quarter dwarfed 204m announced rival google earlier week quarter yahoo earned 187m tipping resurgence online advertising trend taken hold relatively quickly late last year marketing company doubleclick one leading providers online advertising warned business would put sale thursday announced sharp turnaround brought unexpected increase profits neither ask jeeves doubleclick thrilled investors profit news however cases shares fell 4 analysts attributed falls excessive expectations quarters fuelled dramatic outperformance google tuesday\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = df[\"text\"]\n",
    "# headline = df[\"headline\"]\n",
    "# df[\"text\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask\n",
      "Jeeves\n",
      "tips\n",
      "online\n",
      "ad\n",
      "revival\n"
     ]
    }
   ],
   "source": [
    "for sentence in df[\"headline\"][10].split():\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-63d50f808197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# documents = preprocess(df[\"text\"], stopwords_removal=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "documents = preprocess(df[\"text\"], stopwords_removal=True)\n",
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = preprocess(df[\"headline\"])\n",
    "headlines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, documents):\n",
    "    for sentence in documents:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary : 31981\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, documents)\n",
    "count_words(word_counts, headlines)\n",
    "\n",
    "print(\"Size of vocabulary :\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(r'C:\\Users\\amrit\\OneDrive\\Desktop\\Thesis\\Data\\numberbatch-en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 948\n",
      "Percent of words that are missing from vocabulary: 2.96%\n"
     ]
    }
   ],
   "source": [
    "missing_words = 0\n",
    "threshold = 5\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count>threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "missing_ratio = round(missing_words/len(word_counts), 4)*100\n",
    "\n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timewarner', 7),\n",
       " ('76', 14),\n",
       " ('13bn', 6),\n",
       " ('600m', 6),\n",
       " ('11', 240),\n",
       " ('1bn', 82),\n",
       " ('10', 559),\n",
       " ('9bn', 29),\n",
       " ('000', 804),\n",
       " ('2000', 139),\n",
       " ('2003', 387),\n",
       " ('27', 104),\n",
       " ('42', 35),\n",
       " ('2005', 437),\n",
       " ('300m', 14),\n",
       " ('500m', 21),\n",
       " (\"government's\", 130),\n",
       " (\"china's\", 84),\n",
       " (\"us's\", 16),\n",
       " ('rosneft', 59),\n",
       " ('yugansk', 57),\n",
       " ('3bn', 77),\n",
       " ('5bn', 99),\n",
       " (\"yukos'\", 22),\n",
       " ('menatep', 17),\n",
       " (\"group's\", 28),\n",
       " ('40', 163),\n",
       " ('31', 75),\n",
       " ('2004', 628),\n",
       " ('75m', 7)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_words = []\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold and word not in embeddings_index:\n",
    "        missing_words.append((word,count))\n",
    "missing_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 31981\n",
      "Number of words we will use: 25166\n",
      "Percent of words we will use: 78.69%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {}\n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count>= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25166\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return(ints, word_count, unk_count)# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clyde 0 5 celtic\n",
      "ex home secretary david blunkett given fresh clues general election announced monday told bbc radio five live constituency getting ready presume announcement shortly weekend clarified meant would sheffield seat weekend expected election call tony blair tipped ask queen monday dissolve parliament ready 5 may poll\n"
     ]
    }
   ],
   "source": [
    "print(min(headlines, key=len))\n",
    "print((min(documents, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in documents: 508658\n",
      "Total number of UNKs in documents: 10642\n",
      "Percent of words that are UNK: 2.09%\n"
     ]
    }
   ],
   "source": [
    "int_summaries, word_count, unk_count = convert_to_ints(headlines, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(documents, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in documents:\", word_count)\n",
    "print(\"Total number of UNKs in documents:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counts\n",
       "0       6\n",
       "1       5\n",
       "2       6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lengths(int_summaries[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "            counts\n",
      "count  2225.000000\n",
      "mean      5.308764\n",
      "std       0.872975\n",
      "min       3.000000\n",
      "25%       5.000000\n",
      "50%       5.000000\n",
      "75%       6.000000\n",
      "max       9.000000\n",
      "\n",
      "Texts:\n",
      "            counts\n",
      "count  2225.000000\n",
      "mean    224.301573\n",
      "std     128.984200\n",
      "min      46.000000\n",
      "25%     145.000000\n",
      "50%     198.000000\n",
      "75%     275.000000\n",
      "max    2229.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358.0\n",
      "416.0\n",
      "552.5599999999986\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "7.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(min(lengths_texts.counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764\n",
      "764\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 552\n",
    "max_summary_length = 7\n",
    "min_length = 2\n",
    "unk_text_limit = 2\n",
    "unk_summary_limit = 1\n",
    "\n",
    "def filter_condition(item):\n",
    "    int_summary = item[0]\n",
    "    int_text = item[1]\n",
    "    if(len(int_summary) >= min_length and \n",
    "       len(int_summary) <= max_summary_length and \n",
    "       len(int_text) >= min_length and \n",
    "       len(int_text) <= max_text_length and\n",
    "       unk_counter(int_summary) <= unk_summary_limit and \n",
    "       unk_counter(int_text) <= unk_text_limit):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "int_text_summaries = list(zip(int_summaries , int_texts))\n",
    "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
    "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\n",
    "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
    "sorted_summaries = list(sorted_int_text_summaries[0])\n",
    "sorted_texts = list(sorted_int_text_summaries[1])\n",
    "# Delete those temporary varaibles\n",
    "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 66,\n",
       " 68,\n",
       " 69,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 75,\n",
       " 76,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 81,\n",
       " 81]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_texts = [len(text) for text in sorted_texts]\n",
    "lengths_texts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # original code is missing this line below, that is how we connect layers \n",
    "            # by feeding the current layer's output to next layer's input\n",
    "            rnn_inputs = enc_output\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
    "                            vocab_size, max_summary_length,batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<GO>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PAD>' has id: 25163\n",
      "pad summaries batch samples:\n",
      "\r",
      " [[20175 10577 24989   302  1526 25163]\n",
      " [22036 24967  5632 15146 25163 25163]\n",
      " [22036 25028 24991  3410  1908 25163]\n",
      " [10707   272 24963  4848  1526 25163]\n",
      " [20019   367  8498 11004  7208 24968]]\n",
      "robben\n",
      "plays\n",
      "down\n",
      "european\n",
      "return\n",
      "<PAD>\n",
      "capriati\n",
      "to\n",
      "miss\n",
      "melbourne\n",
      "<PAD>\n",
      "<PAD>\n",
      "capriati\n",
      "out\n",
      "of\n",
      "australian\n",
      "open\n",
      "<PAD>\n",
      "collins\n",
      "calls\n",
      "for\n",
      "chambers\n",
      "return\n",
      "<PAD>\n",
      "cole\n",
      "faces\n",
      "lengthy\n",
      "injury\n",
      "lay\n",
      "off\n"
     ]
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
    "sorted_summaries_samples = sorted_summaries[7:50]\n",
    "sorted_texts_samples = sorted_texts[7:50]\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
    "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))\n",
    "for i in range(len(pad_summaries_batch_samples)):\n",
    "    for j in range(len(pad_summaries_batch_samples[i])):\n",
    "        print(int_to_vocab[pad_summaries_batch_samples[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 5\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "# reset_graph()\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 46\n",
      "The longest text length: 504\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "sorted_summaries_short = sorted_summaries\n",
    "sorted_texts_short = sorted_texts\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch   20/152 - Loss:  9.934, Seconds: 26.64\n",
      "Epoch   1/100 Batch   40/152 - Loss:  8.015, Seconds: 24.00\n",
      "Average loss for this update: 8.723\n",
      "New Record!\n",
      "Epoch   1/100 Batch   60/152 - Loss:  7.723, Seconds: 26.36\n",
      "Epoch   1/100 Batch   80/152 - Loss:  8.088, Seconds: 29.24\n",
      "Average loss for this update: 7.795\n",
      "New Record!\n",
      "Epoch   1/100 Batch  100/152 - Loss:  7.497, Seconds: 33.68\n",
      "Epoch   1/100 Batch  120/152 - Loss:  7.732, Seconds: 46.00\n",
      "Epoch   1/100 Batch  140/152 - Loss:  7.040, Seconds: 54.76\n",
      "Average loss for this update: 7.409\n",
      "New Record!\n",
      "Epoch   2/100 Batch   20/152 - Loss:  6.333, Seconds: 20.63\n",
      "Epoch   2/100 Batch   40/152 - Loss:  5.838, Seconds: 23.76\n",
      "Average loss for this update: 5.969\n",
      "New Record!\n",
      "Epoch   2/100 Batch   60/152 - Loss:  5.606, Seconds: 26.46\n",
      "Epoch   2/100 Batch   80/152 - Loss:  5.972, Seconds: 31.44\n",
      "Average loss for this update: 5.816\n",
      "New Record!\n",
      "Epoch   2/100 Batch  100/152 - Loss:  5.753, Seconds: 36.40\n",
      "Epoch   2/100 Batch  120/152 - Loss:  6.022, Seconds: 45.13\n",
      "Epoch   2/100 Batch  140/152 - Loss:  5.643, Seconds: 51.93\n",
      "Average loss for this update: 5.858\n",
      "No Improvement.\n",
      "Epoch   3/100 Batch   20/152 - Loss:  6.073, Seconds: 20.78\n",
      "Epoch   3/100 Batch   40/152 - Loss:  5.562, Seconds: 24.14\n",
      "Average loss for this update: 5.699\n",
      "New Record!\n",
      "Epoch   3/100 Batch   60/152 - Loss:  5.346, Seconds: 26.35\n",
      "Epoch   3/100 Batch   80/152 - Loss:  5.611, Seconds: 31.42\n",
      "Average loss for this update: 5.49\n",
      "New Record!\n",
      "Epoch   3/100 Batch  100/152 - Loss:  5.408, Seconds: 34.36\n",
      "Epoch   3/100 Batch  120/152 - Loss:  5.687, Seconds: 69.71\n",
      "Epoch   3/100 Batch  140/152 - Loss:  5.382, Seconds: 86.38\n",
      "Average loss for this update: 5.549\n",
      "No Improvement.\n",
      "Epoch   4/100 Batch   20/152 - Loss:  5.972, Seconds: 32.35\n",
      "Epoch   4/100 Batch   40/152 - Loss:  5.262, Seconds: 41.66\n",
      "Average loss for this update: 5.526\n",
      "No Improvement.\n",
      "Epoch   4/100 Batch   60/152 - Loss:  5.194, Seconds: 42.68\n",
      "Epoch   4/100 Batch   80/152 - Loss:  5.416, Seconds: 52.27\n",
      "Average loss for this update: 5.309\n",
      "New Record!\n",
      "Epoch   4/100 Batch  100/152 - Loss:  5.276, Seconds: 41.01\n",
      "Epoch   4/100 Batch  120/152 - Loss:  5.418, Seconds: 71.90\n",
      "Epoch   4/100 Batch  140/152 - Loss:  5.121, Seconds: 89.40\n",
      "Average loss for this update: 5.303\n",
      "New Record!\n",
      "Epoch   5/100 Batch   20/152 - Loss:  5.712, Seconds: 20.96\n",
      "Epoch   5/100 Batch   40/152 - Loss:  5.151, Seconds: 23.41\n",
      "Average loss for this update: 5.359\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch   60/152 - Loss:  5.145, Seconds: 43.45\n",
      "Epoch   5/100 Batch   80/152 - Loss:  5.459, Seconds: 53.74\n",
      "Average loss for this update: 5.246\n",
      "New Record!\n",
      "Epoch   5/100 Batch  100/152 - Loss:  5.047, Seconds: 37.40\n",
      "Epoch   5/100 Batch  120/152 - Loss:  5.317, Seconds: 49.76\n",
      "Epoch   5/100 Batch  140/152 - Loss:  5.113, Seconds: 87.02\n",
      "Average loss for this update: 5.242\n",
      "New Record!\n",
      "Epoch   6/100 Batch   20/152 - Loss:  5.739, Seconds: 19.26\n",
      "Epoch   6/100 Batch   40/152 - Loss:  5.130, Seconds: 40.48\n",
      "Average loss for this update: 5.352\n",
      "No Improvement.\n",
      "Epoch   6/100 Batch   60/152 - Loss:  5.149, Seconds: 44.69\n",
      "Epoch   6/100 Batch   80/152 - Loss:  5.495, Seconds: 51.00\n",
      "Average loss for this update: 5.269\n",
      "No Improvement.\n",
      "Epoch   6/100 Batch  100/152 - Loss:  5.085, Seconds: 58.68\n",
      "Epoch   6/100 Batch  120/152 - Loss:  5.355, Seconds: 70.09\n",
      "Epoch   6/100 Batch  140/152 - Loss:  5.161, Seconds: 84.13\n",
      "Average loss for this update: 5.307\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = ' '.join(text)\n",
    "    tokens = text.split()\n",
    "    words = []\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    for token in tokens:\n",
    "        if token not in cachedStopWords:\n",
    "            words.append(token)\n",
    "    text = \" \".join(words)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faultless federer has no equal'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \" \".join([int_to_vocab[i] for i in sorted_texts[500]])\n",
    "b = \" \".join([int_to_vocab[i] for i in sorted_texts[550]])\n",
    "\" \".join([int_to_vocab[i] for i in sorted_summaries[550]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "- Review:\n",
      " michael moore's anti bush documentary fahrenheit 9 11 best film us people's choice awards voted us public mel gibson's passion christ best drama despite films snubbed far us film awards run february's oscars julia roberts 10th consecutive crown favourite female movie star johnny depp favourite male movie star renee zellweger favourite leading lady sunday's awards la film sequel shrek 2 took three prizes voted top animated movie top film comedy top sequel television categories desperate housewives named top new drama joey starring former friends actor matt leblanc best new comedy long running shows grace csi crime scene investigation named best tv comedy tv drama respectively nominees people's choice awards picked 6 000 strong entertainment weekly magazine panel winners subsequently chosen 21 million online voters fahrenheit 9 11 director michael moore dedicated trophy soldiers iraq film highly critical president george w bush us led invasion iraq moore outspoken bush critic 2004 presidential campaign <UNK> democratic challenger john kerry lost country still right left democrat republican moore told audience ceremony pasadena california moore said historic occasion 31 year old awards ceremony would name documentary best film unlike many film makers passion christ director mel gibson vowed campaign oscar movie really ultimate goal one make work elite gibson said backstage event people spoken <EOS>\n",
      "[ 4620 23745 24963  2198 25163]\n",
      "- Summary:\n",
      " blair pickings for plan\n",
      "\n",
      "\n",
      "- Review:\n",
      " roger federer nice bloke fantastic tennis player ultimate sportsman lleyton hewitt shook hand getting another thrashing third many months australian said best right stats speak 11 titles 11 finals 2004 three grand slams 13 final victories row going back vienna 2003 open era record hewitt times houston showed form easily matched grand slam winning efforts 2001 2002 outplayed twice hewitt along andy roddick marat safin sure prominent 2005 realistically three fighting world number two ranking according players even federer swiss star different league right feel little bit told bbc sport dominated top ten players say nice things beaten dominating game right hope continues number one player world also main man promoting sport court voted international tennis writers best ambassador tennis atp tour time everyone every match first round final followed series press interviews three languages english french swiss german major win extra requests obligations interviews seen end courtesy importantly good humour guys funny good time guys said genuinely happy talk yet another tape recorder see pretty much every day tour give away hour interviews really problem promote tennis sport good people say thanks back nice refreshing attitude someone could easily dominate sports pages decade sums modest personality shortly collecting waterford crystal trophy mercedes convertible tasty cheque 1 5m federer addressed houston crowd concluded saying thanks needs find way winning french open one grand slam far elude <EOS>\n",
      "[ 4620  9729 24963  9971 25163]\n",
      "- Summary:\n",
      " blair handed for unity\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [a, b]\n",
    "generagte_summary_length =  [5,5]\n",
    "\n",
    "texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "if type(generagte_summary_length) is list:\n",
    "    if len(input_sentences)!=len(generagte_summary_length):\n",
    "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n",
    "    generagte_summary_length_list = generagte_summary_length\n",
    "else:\n",
    "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i, text in enumerate(texts):\n",
    "        generagte_summary_length = generagte_summary_length_list[i]\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "        # Remove the padding from the summaries\n",
    "        pad = vocab_to_int[\"<PAD>\"] \n",
    "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n",
    "        print(answer_logits)\n",
    "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
